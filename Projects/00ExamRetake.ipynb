{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quant Trading and Analysis with Python: Final Exam 2021 (Retake)\n",
    "\n",
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Many academic papers report a so-called low risk anomaly, e.g., betting against beta, etc.\n",
    "\n",
    "Your task is to provide a simple test of such anomaly using a particular trading strategy:\n",
    "\n",
    "* Using Pandas Datareader download daily returns from 01.01.2000 for 25 Fama-French portfolios\n",
    "sorted by the ME and BTM characteristics `25_Portfolios_5x5_Daily` (Rename the portfolios as `ptf1` to `ptf25`).\n",
    "Organize returns into a data frame indexed by dates. Download daily returns for the 3-factor model (market, size\n",
    "value) from 01.01.2000.\n",
    "\n",
    "* For each portfolio on each day compute the past volatility, and the average past correlation with other portfolios\n",
    "over the past 21 trading days (use window = 21). Check how the levels of past volatility and average correlations are\n",
    "related for each pair of stocks.\n",
    "\n",
    "* Using a simple unconditional cross-sectional test, check how average returns are related to the average 21-day volatility\n",
    "and average 21-day correlation computed in the previous task.\n",
    "\n",
    "* Instead of a regression, test the same claim using daily rebalanced zero-cost portfolios, in which on each day you go long\n",
    " stocks with the value of characteristic in the top 20%, and short -- stocks with the value of characteristic in the bottom 20%.\n",
    " Use equal weighting for both long and short sides. Use separately average volatility and average correlation computed on each day to rebalance portfolios after the same day's close.\n",
    "Compute the average return, and Sharpe ratios of both portfolios.\n",
    "\n",
    "* Discuss the results. Reflect on your evidence about the low risk anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pandas_datareader as web\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%% #########  Q1\n"
    }
   },
   "outputs": [],
   "source": [
    "startdt  = datetime.datetime(2000, 1, 1)\n",
    "# sets = web.famafrench.get_available_datasets()\n",
    "\n",
    "d1          = web.DataReader('25_Portfolios_5x5_Daily','famafrench',start = startdt)\n",
    "ret         = d1[0]/100 # save them just in case\n",
    "ret.columns = ['ptf' + str(z) for z in np.arange(1,ret.shape[1]+1)]\n",
    "\n",
    "d1  = web.DataReader('F-F_Research_Data_Factors','famafrench',start = startdt)\n",
    "ff  = pd.DataFrame(d1[0]/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%  #########  Q2\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ptf1      ptf2      ptf3      ptf4      ptf5      ptf6  \\\n",
      "Date                                                                     \n",
      "2000-02-01  0.605859  0.647110  0.610917  0.743167  0.657495  0.707407   \n",
      "2000-02-02  0.635544  0.638084  0.680411  0.735563  0.653285  0.701130   \n",
      "2000-02-03  0.547596  0.559851  0.607489  0.692102  0.590851  0.637303   \n",
      "2000-02-04  0.527869  0.554290  0.604135  0.686439  0.588710  0.629772   \n",
      "2000-02-07  0.524813  0.549771  0.608247  0.690344  0.591095  0.629449   \n",
      "\n",
      "                ptf7      ptf8      ptf9     ptf10  ...     ptf16     ptf17  \\\n",
      "Date                                                ...                       \n",
      "2000-02-01  0.701203  0.716835  0.650754  0.726479  ...  0.678073  0.722287   \n",
      "2000-02-02  0.695804  0.707227  0.638016  0.721645  ...  0.697756  0.720999   \n",
      "2000-02-03  0.628213  0.656494  0.535786  0.660808  ...  0.650755  0.654750   \n",
      "2000-02-04  0.628331  0.646950  0.535854  0.660855  ...  0.662062  0.652767   \n",
      "2000-02-07  0.641813  0.657482  0.534825  0.667839  ...  0.709024  0.656233   \n",
      "\n",
      "               ptf18     ptf19     ptf20     ptf21     ptf22     ptf23  \\\n",
      "Date                                                                     \n",
      "2000-02-01  0.654399  0.610153  0.663584  0.633385  0.604519  0.440657   \n",
      "2000-02-02  0.645960  0.582442  0.654217  0.664526  0.583855  0.403583   \n",
      "2000-02-03  0.577446  0.503637  0.601176  0.589151  0.473758  0.274088   \n",
      "2000-02-04  0.579435  0.502195  0.601482  0.593460  0.470109  0.269419   \n",
      "2000-02-07  0.600401  0.516691  0.595255  0.610598  0.478107  0.301369   \n",
      "\n",
      "               ptf24     ptf25  \n",
      "Date                            \n",
      "2000-02-01  0.566251  0.568132  \n",
      "2000-02-02  0.527133  0.546575  \n",
      "2000-02-03  0.424015  0.473675  \n",
      "2000-02-04  0.425966  0.470919  \n",
      "2000-02-07  0.477644  0.470645  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "[0.2669540966985969, 0.25953469758796244, 0.40779809688316315, 0.3817693987481684, 0.11586553801791714, 0.2675512767748134, 0.3774913444486216, 0.4172980628714932, 0.36290740593901777, 0.3894982354288098, 0.1844551071139257, 0.36331823442819006, 0.37255455706376567, 0.3838580299897766, 0.3190428260192078, 0.1543973195307806, 0.390904606848254, 0.40417718124994273, 0.3455796507280772, 0.3340767174179183, 0.25176843371717395, 0.36473563041681395, 0.31532582948413174, 0.2873568934778166, 0.2799505236085335]\n"
     ]
    }
   ],
   "source": [
    "vols = ret.rolling(window = 21).std().dropna()\n",
    "corrs = ret.rolling(window = 21).corr().mean(axis = 1).unstack()-1/25 # sime adjustment for diagonal of 1\n",
    "corrs = corrs.reindex_like(vols)\n",
    "\n",
    "print(corrs.head())\n",
    "\n",
    "clevel = [np.corrcoef(x = vols[z], y = corrs[z])[0,1] for z in vols.columns]\n",
    "print(clevel)\n",
    "\n",
    "# the correlations in levels are positive, but not extremely high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:     Mean returns, p.a.   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.043\n",
      "Method:                 Least Squares   F-statistic:                  0.002021\n",
      "Date:                Fri, 07 May 2021   Prob (F-statistic):              0.965\n",
      "Time:                        15:44:59   Log-Likelihood:                 58.361\n",
      "No. Observations:                  25   AIC:                            -112.7\n",
      "Df Residuals:                      23   BIC:                            -110.3\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "const                  0.1198      0.046      2.598      0.016       0.024       0.215\n",
      "Average Volatility     0.0102      0.227      0.045      0.965      -0.459       0.479\n",
      "==============================================================================\n",
      "Omnibus:                        5.770   Durbin-Watson:                   1.334\n",
      "Prob(Omnibus):                  0.056   Jarque-Bera (JB):                4.138\n",
      "Skew:                          -0.974   Prob(JB):                        0.126\n",
      "Kurtosis:                       3.422   Cond. No.                         48.3\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:     Mean returns, p.a.   R-squared:                       0.482\n",
      "Model:                            OLS   Adj. R-squared:                  0.460\n",
      "Method:                 Least Squares   F-statistic:                     21.41\n",
      "Date:                Fri, 07 May 2021   Prob (F-statistic):           0.000118\n",
      "Time:                        15:44:59   Log-Likelihood:                 66.585\n",
      "No. Observations:                  25   AIC:                            -129.2\n",
      "Df Residuals:                      23   BIC:                            -126.7\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "const                  -0.2772      0.086     -3.212      0.004      -0.456      -0.099\n",
      "Average Correlation     0.4938      0.107      4.628      0.000       0.273       0.714\n",
      "==============================================================================\n",
      "Omnibus:                        1.667   Durbin-Watson:                   2.263\n",
      "Prob(Omnibus):                  0.435   Jarque-Bera (JB):                0.769\n",
      "Skew:                          -0.414   Prob(JB):                        0.681\n",
      "Kurtosis:                       3.228   Cond. No.                         50.2\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "mret = ret.mean(axis = 0)*251\n",
    "mret.name = 'Mean returns, p.a.'# annualize for simpler interpretation\n",
    "mvols = vols.mean(axis = 0)*np.sqrt(251)\n",
    "mvols.name = 'Average Volatility'\n",
    "mcorrs = corrs.mean(axis = 0)\n",
    "mcorrs.name = 'Average Correlation'\n",
    "\n",
    "#\n",
    "mvols = sm.add_constant(mvols)\n",
    "mcorrs = sm.add_constant(mcorrs)\n",
    "\n",
    "#\n",
    "resvols = sm.OLS(mret, mvols).fit()\n",
    "rescorrs = sm.OLS(mret, mcorrs).fit()\n",
    "\n",
    "#\n",
    "print(resvols.summary())\n",
    "print(rescorrs.summary())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% #########  Q3\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results are as follows:\n",
    "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
    "\n",
    "Average Volatility     0.0102      0.227      0.045      0.965      -0.459       0.479\n",
    "\n",
    "Average Correlation     0.4938      0.107      4.628      0.000       0.273       0.714\n",
    "\n",
    "So the average volatility does not seem to be related to returns, but the average correlation has a strong\n",
    "positive relation to returns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%% #########  Q4\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                  ret_fw      vols     corrs\nDate                                        \n2000-02-01 ptf1   0.0119  0.020788  0.605859\n           ptf10  0.0166  0.013730  0.726479\n           ptf11  0.0213  0.024786  0.716138\n           ptf12  0.0103  0.018776  0.749728\n           ptf13  0.0018  0.012780  0.640608\n...                  ...       ...       ...\n2021-03-30 ptf5   0.0073  0.032028  0.670461\n           ptf6   0.0306  0.029019  0.665896\n           ptf7   0.0141  0.022665  0.776958\n           ptf8   0.0107  0.020406  0.783966\n           ptf9  -0.0006  0.019419  0.710139\n\n[133100 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>ret_fw</th>\n      <th>vols</th>\n      <th>corrs</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2000-02-01</th>\n      <th>ptf1</th>\n      <td>0.0119</td>\n      <td>0.020788</td>\n      <td>0.605859</td>\n    </tr>\n    <tr>\n      <th>ptf10</th>\n      <td>0.0166</td>\n      <td>0.013730</td>\n      <td>0.726479</td>\n    </tr>\n    <tr>\n      <th>ptf11</th>\n      <td>0.0213</td>\n      <td>0.024786</td>\n      <td>0.716138</td>\n    </tr>\n    <tr>\n      <th>ptf12</th>\n      <td>0.0103</td>\n      <td>0.018776</td>\n      <td>0.749728</td>\n    </tr>\n    <tr>\n      <th>ptf13</th>\n      <td>0.0018</td>\n      <td>0.012780</td>\n      <td>0.640608</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2021-03-30</th>\n      <th>ptf5</th>\n      <td>0.0073</td>\n      <td>0.032028</td>\n      <td>0.670461</td>\n    </tr>\n    <tr>\n      <th>ptf6</th>\n      <td>0.0306</td>\n      <td>0.029019</td>\n      <td>0.665896</td>\n    </tr>\n    <tr>\n      <th>ptf7</th>\n      <td>0.0141</td>\n      <td>0.022665</td>\n      <td>0.776958</td>\n    </tr>\n    <tr>\n      <th>ptf8</th>\n      <td>0.0107</td>\n      <td>0.020406</td>\n      <td>0.783966</td>\n    </tr>\n    <tr>\n      <th>ptf9</th>\n      <td>-0.0006</td>\n      <td>0.019419</td>\n      <td>0.710139</td>\n    </tr>\n  </tbody>\n</table>\n<p>133100 rows Ã— 3 columns</p>\n</div>"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the data into one panel\n",
    "retl = ret.shift(-1).stack() # change to tomorrow's return\n",
    "retl.name = 'ret_fw'\n",
    "\n",
    "volsl = vols.stack()\n",
    "volsl.name = 'vols'\n",
    "\n",
    "corrsl = corrs.stack()\n",
    "corrsl.name = 'corrs'\n",
    "\n",
    "data = pd.concat([retl,volsl, corrsl], axis = 1).dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def long_short_ptf(data, charact, perf_ret = 'ret_fw', perc = 20):\n",
    "    col_name = perf_ret\n",
    "    l,h = np.percentile(data[charact].values, [perc, 100-perc])\n",
    "    longs_ = data[data[charact] >= h]\n",
    "    shorts_ = data[data[charact] <= l]\n",
    "    res = np.append(longs_[col_name].values / longs_.index.size, - shorts_[col_name].values / shorts_.index.size)\n",
    "    if len(res) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return res.sum()\n",
    "#\n",
    "str_vols = data.groupby(['Date']).apply(lambda x: long_short_ptf(data = x, charact = 'vols'))\n",
    "str_corrs = data.groupby(['Date']).apply(lambda x: long_short_ptf(data = x, charact = 'corrs'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% define a function for portoflio performance computation/ evaluate\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vola-based performance: Mean -4.74 % p.a., Volatility  14.73 % p.a., SR -0.32\n",
      "Corrs-based performance: Mean  0.76 % p.a., Volatility  9.07 % p.a., SR  0.08\n"
     ]
    }
   ],
   "source": [
    "mn_vols, vol_vols =  str_vols.mean()*252, str_vols.std()*np.sqrt(252)\n",
    "mn_corrs, vol_corrs =  str_corrs.mean()*252, str_corrs.std()*np.sqrt(252)\n",
    "\n",
    "print(f'Vola-based performance: Mean {mn_vols*100: .2f} % p.a., Volatility {vol_vols*100: .2f} % p.a., SR {mn_vols/vol_vols: .2f}')\n",
    "print(f'Corrs-based performance: Mean {mn_corrs*100: .2f} % p.a., Volatility {vol_corrs*100: .2f} % p.a., SR {mn_corrs/vol_corrs: .2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% check the performance\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#########  Q5\n",
    "\n",
    "Both higher volatility and higher correlations with the other stocks indicate higher risk, and shall be compensated\n",
    "by expected returns. Using unconditional cross-sectional tests we observe that correlation is positively related\n",
    "to returns in the cross-section, while volatility is insignificant. Using a conditional strategy with very simple rules,\n",
    "we find a negative link for volatility, and a very wek positive link for correlations. It seems that for this particular\n",
    "choice of parameters a conditional strategy of betting against higher risk might work, though we need more tests to\n",
    "make a conclusion. We would need to see the factor exposure of the resulting strategy, check different rebalancing and\n",
    "weighting rules, and, certainly, establish the significance of the strategy performance in statistical terms.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "201.577px",
    "left": "1737.46px",
    "right": "20px",
    "top": "112.918px",
    "width": "380.185px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}