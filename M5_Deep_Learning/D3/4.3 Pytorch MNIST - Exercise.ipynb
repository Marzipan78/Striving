{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\">Deep Learning Fundamentals</h2>\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\">Multiclass Classification: MNIST</h2>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:36.081105Z",
     "start_time": "2021-05-26T22:26:35.040138Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxliary plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:37.473177Z",
     "start_time": "2021-05-26T22:26:37.465910Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/view-classify-in-module-helper/30279/6\n",
    "\n",
    "\n",
    "def view_classify(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Dataset\n",
    "First up, we need to get our dataset. This is provided through the `torchvision` package. The code below will download the MNIST dataset, then create training and test datasets for us. Don't worry too much about the details here, you'll learn more about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:38.402766Z",
     "start_time": "2021-05-26T22:26:38.298968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data (Preprocessing)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5)) ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:38.632988Z",
     "start_time": "2021-05-26T22:26:38.477558Z"
    }
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. We'd use this to loop through the dataset for training, but here I'm just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size (64, 1, 28, 28). So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:39.407000Z",
     "start_time": "2021-05-26T22:26:39.265256Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 251,
       "height": 248
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch\n",
    "\n",
    "Here I'll use PyTorch to build a simple feedfoward network to classify the MNIST images. That is, the network will receive a digit image as input and predict the digit in the image.\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width=600px>\n",
    "\n",
    "To build a neural network with PyTorch, you use the `torch.nn` module. The network itself is a class inheriting from `torch.nn.Module`. You define each of the operations separately, like `nn.Linear(784, 128)` for a fully connected linear layer with 784 inputs and 128 units.\n",
    "\n",
    "The class needs to include a `forward` method that implements the forward pass through the network. In this method, you pass some input tensor `x` through each of the operations you defined earlier. The `torch.nn` module also has functional equivalents for things like ReLUs in `torch.nn.functional`. This module is usually imported as `F`. Then to use a ReLU activation on some layer (which is just a tensor), you'd do `F.relu(x)`. Below are a few different commonly used activation functions.\n",
    "\n",
    "<img src=\"assets/activation.png\" width=700px>\n",
    "\n",
    "So, for this network, I'll build it with three fully connected layers, then a softmax output for predicting classes. The softmax function is similar to the sigmoid in that it squashes inputs between 0 and 1, but it's also normalized so that all the values sum to one like a proper probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:39.961531Z",
     "start_time": "2021-05-26T22:26:39.946776Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    # Defining the layers, 128, 64, 10 units each\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 10)\n",
    "        \n",
    "    # Forward pass through the network, returns the output logits\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the input features are 784? Because the input images have size 28 pixels x 28 pixels for a total of 784 features. Since a Multilayer perceptron accepts only flatten inputs, we need to flatten a 28x28 grid into a 784 array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential API\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:41.213448Z",
     "start_time": "2021-05-26T22:26:41.205216Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Linear(in_features=784, out_features=4, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=4, out_features=4, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=4, out_features=10, bias=True)\n  (5): Softmax(dim=1)\n)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [4, 4]\n",
    "output_size   = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in an `OrderedDict` to name the individual layers and operations. Note that a dictionary keys must be unique, so _each operation must have a different name_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:42.300216Z",
     "start_time": "2021-05-26T22:26:42.289009Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=4, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=4, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1',   nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2',   nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "          ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and biases\n",
    "\n",
    "The weights and such are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with `model.fc1.weight` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:42.972699Z",
     "start_time": "2021-05-26T22:26:42.963913Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[ 0.0089, -0.0018,  0.0241,  ..., -0.0310,  0.0173,  0.0278],\n        [-0.0002,  0.0303,  0.0166,  ...,  0.0003, -0.0215, -0.0197],\n        [-0.0026,  0.0266, -0.0289,  ...,  0.0036,  0.0209, -0.0209],\n        [-0.0063, -0.0148, -0.0017,  ...,  0.0311, -0.0244,  0.0032]],\n       requires_grad=True)\nParameter containing:\ntensor([-0.0008, -0.0116, -0.0013,  0.0008], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For custom initialization, we want to modify these tensors in place. These are actually autograd *Variables*, so we need to get back the actual tensors with `model.fc1.weight.data`. Once we have the tensors, we can fill them with zeros (for biases) or random normal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:43.889729Z",
     "start_time": "2021-05-26T22:26:43.883940Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Set biases to all zeros\n",
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:44.084097Z",
     "start_time": "2021-05-26T22:26:44.076738Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.0140,  0.0151, -0.0081,  ...,  0.0155, -0.0142, -0.0116],\n",
       "        [-0.0129, -0.0088,  0.0132,  ...,  0.0138,  0.0116, -0.0115],\n",
       "        [ 0.0192,  0.0113, -0.0208,  ..., -0.0094, -0.0166,  0.0057],\n",
       "        [ 0.0188, -0.0115, -0.0030,  ..., -0.0043, -0.0030, -0.0010]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Forward pass\n",
    "\n",
    "Now that we have a network, let's see what happens when we pass in an image. This is called the forward pass. We're going to convert the image data into a tensor, then pass it through the operations defined by the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:44.506324Z",
     "start_time": "2021-05-26T22:26:44.491847Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grab some data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:44.596541Z",
     "start_time": "2021-05-26T22:26:44.594169Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:44.851894Z",
     "start_time": "2021-05-26T22:26:44.845888Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=4, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=4, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:46.121246Z",
     "start_time": "2021-05-26T22:26:46.112022Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 784])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "images.shape\n",
    "# or images.resize_(images.shape[0], 1, 784) to not automatically get batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:46.519895Z",
     "start_time": "2021-05-26T22:26:46.514137Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "img_idx = 0\n",
    "images[img_idx,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:47.945952Z",
     "start_time": "2021-05-26T22:26:47.888846Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Forward pass through the network\n",
    "img_idx = 0\n",
    "ps = model(images[img_idx,:])\n",
    "ps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:50.561845Z",
     "start_time": "2021-05-26T22:26:50.411449Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "img = images[img_idx]\n",
    "img.shape\n",
    "#view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our network has basically no idea what this digit is. It's because we haven't trained it yet, all the weights are random!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "The network we built isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
    "\n",
    "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "For single layer networks, gradient descent is simple to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks, although it's straightforward once you learn about it. \n",
    "\n",
    "This is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/w1_backprop_graph.png' width=400px>\n",
    "\n",
    "In the forward pass through the network, our data and operations go from right to left here. To train the weights with gradient descent, we propagate the gradient of the cost backwards through the network. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial w_1} = \\frac{\\partial l_1}{\\partial w_1} \\frac{\\partial s}{\\partial l_1} \\frac{\\partial l_2}{\\partial s} \\frac{\\partial \\ell}{\\partial l_2}\n",
    "$$\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "w^\\prime = w - \\alpha \\frac{\\partial \\ell}{\\partial w}\n",
    "$$\n",
    "\n",
    "The learning rate is set such that the weight update steps are small enough that the iterative method settles in a minimum.\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Torch provides a module, `autograd`, for automatically calculating the gradient of tensors. It does this by keeping track of operations performed on tensors. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`.\n",
    "\n",
    "You can turn off gradients for a block of code with the `torch.no_grad()` content:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:52.867509Z",
     "start_time": "2021-05-26T22:26:52.860629Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.8157,  0.2951],\n        [-1.1629, -0.1559]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:53.383436Z",
     "start_time": "2021-05-26T22:26:53.375536Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.6654, 0.0871],\n        [1.3522, 0.0243]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the operation that created `y`, a power operation `PowBackward0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:53.870654Z",
     "start_time": "2021-05-26T22:26:53.867424Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PowBackward0 object at 0x000001DB6F3A5888>\n"
     ]
    }
   ],
   "source": [
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autgrad module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor. Let's reduce the tensor `y` to a scalar value, the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:54.831912Z",
     "start_time": "2021-05-26T22:26:54.824631Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.5323, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the gradients for `x` and `y` but they are empty currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:55.546143Z",
     "start_time": "2021-05-26T22:26:55.541213Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradients, you need to run the `.backward` method on a Variable, `z` for example. This will calculate the gradient for `z` with respect to `x`\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:56.607560Z",
     "start_time": "2021-05-26T22:26:56.594993Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.4079,  0.1476],\n        [-0.5814, -0.0780]])\ntensor([[ 0.4079,  0.1476],\n        [-0.5814, -0.0780]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the cost, then, go backwards to calculate the gradients with respect to the cost. Once we have the gradients we can make a gradient descent step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll build a network with `nn.Sequential` here. Only difference from the last part is I'm not actually using softmax on the output, but instead just using the raw output from the last layer. This is because the output from softmax is a probability distribution. Often, the output will have values really close to zero or really close to one. Due to [inaccuracies with representing numbers as floating points](https://docs.python.org/3/tutorial/floatingpoint.html), computations with a softmax output can lose accuracy and become unstable. To get around this, we'll use the raw output, called the **logits**, to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:56.944759Z",
     "start_time": "2021-05-26T22:26:56.936939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size  = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('logits', nn.Linear(hidden_sizes[1], output_size))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network!\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:57.317614Z",
     "start_time": "2021-05-26T22:26:57.313022Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider just one learning step before looping through all the data. The general process with PyTorch:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "Below I'll go through one training step and print out the weights and gradients so you can see how it changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:27:07.408433Z",
     "start_time": "2021-05-26T22:27:07.373358Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial weights -  Parameter containing:\ntensor([[-0.0025, -0.0103,  0.0089,  ..., -0.0147,  0.0070, -0.0319],\n        [-0.0178,  0.0116, -0.0290,  ..., -0.0347, -0.0145, -0.0033],\n        [ 0.0188, -0.0068,  0.0307,  ...,  0.0047,  0.0046,  0.0280],\n        ...,\n        [-0.0118, -0.0055, -0.0166,  ...,  0.0219,  0.0288,  0.0004],\n        [-0.0075,  0.0267, -0.0260,  ..., -0.0069, -0.0175, -0.0141],\n        [-0.0209, -0.0045,  0.0087,  ..., -0.0345, -0.0181,  0.0240]],\n       requires_grad=True)\nGradient - tensor([[-0.0057, -0.0057, -0.0057,  ..., -0.0057, -0.0057, -0.0057],\n        [ 0.0039,  0.0039,  0.0039,  ...,  0.0039,  0.0039,  0.0039],\n        [ 0.0033,  0.0033,  0.0033,  ...,  0.0033,  0.0033,  0.0033],\n        ...,\n        [-0.0018, -0.0018, -0.0018,  ..., -0.0018, -0.0018, -0.0018],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model.fc1.weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(16, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:27:07.915247Z",
     "start_time": "2021-05-26T22:27:07.908155Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Updated weights -  Parameter containing:\ntensor([[-0.0025, -0.0103,  0.0089,  ..., -0.0146,  0.0070, -0.0319],\n        [-0.0178,  0.0115, -0.0291,  ..., -0.0347, -0.0145, -0.0034],\n        [ 0.0187, -0.0069,  0.0307,  ...,  0.0047,  0.0045,  0.0280],\n        ...,\n        [-0.0118, -0.0055, -0.0166,  ...,  0.0219,  0.0288,  0.0005],\n        [-0.0075,  0.0267, -0.0260,  ..., -0.0069, -0.0175, -0.0141],\n        [-0.0209, -0.0045,  0.0087,  ..., -0.0345, -0.0181,  0.0240]],\n       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Updated weights - ', model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for real\n",
    "\n",
    "Now we'll put this algorithm into a loop so we can go through all the images. This is fairly straightforward. We'll loop through the mini-batches in our dataset, pass the data through the network to calculate the losses, get the gradients, then run the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:27:08.816179Z",
     "start_time": "2021-05-26T22:27:08.812807Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:27:36.083537Z",
     "start_time": "2021-05-26T22:27:09.280769Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/3\n",
      "\tIteration: 0\t Loss: 0.0577\n",
      "\tIteration: 40\t Loss: 2.2840\n",
      "\tIteration: 80\t Loss: 2.2711\n",
      "\tIteration: 120\t Loss: 2.2571\n",
      "\tIteration: 160\t Loss: 2.2302\n",
      "\tIteration: 200\t Loss: 2.2067\n",
      "\tIteration: 240\t Loss: 2.1861\n",
      "\tIteration: 280\t Loss: 2.1650\n",
      "\tIteration: 320\t Loss: 2.1382\n",
      "\tIteration: 360\t Loss: 2.1110\n",
      "\tIteration: 400\t Loss: 2.0691\n",
      "\tIteration: 440\t Loss: 2.0382\n",
      "\tIteration: 480\t Loss: 1.9988\n",
      "\tIteration: 520\t Loss: 1.9117\n",
      "\tIteration: 560\t Loss: 1.8897\n",
      "\tIteration: 600\t Loss: 1.8738\n",
      "\tIteration: 640\t Loss: 1.7889\n",
      "\tIteration: 680\t Loss: 1.7151\n",
      "\tIteration: 720\t Loss: 1.6406\n",
      "\tIteration: 760\t Loss: 1.5777\n",
      "\tIteration: 800\t Loss: 1.4998\n",
      "\tIteration: 840\t Loss: 1.4850\n",
      "\tIteration: 880\t Loss: 1.3949\n",
      "\tIteration: 920\t Loss: 1.3491\n",
      "\tIteration: 960\t Loss: 1.2896\n",
      "\tIteration: 1000\t Loss: 1.2534\n",
      "\tIteration: 1040\t Loss: 1.2355\n",
      "\tIteration: 1080\t Loss: 1.1764\n",
      "\tIteration: 1120\t Loss: 1.0870\n",
      "\tIteration: 1160\t Loss: 1.0445\n",
      "\tIteration: 1200\t Loss: 0.9945\n",
      "\tIteration: 1240\t Loss: 0.9518\n",
      "\tIteration: 1280\t Loss: 0.9453\n",
      "\tIteration: 1320\t Loss: 0.9041\n",
      "\tIteration: 1360\t Loss: 0.8860\n",
      "\tIteration: 1400\t Loss: 0.8222\n",
      "\tIteration: 1440\t Loss: 0.8097\n",
      "\tIteration: 1480\t Loss: 0.8222\n",
      "\tIteration: 1520\t Loss: 0.7764\n",
      "\tIteration: 1560\t Loss: 0.7360\n",
      "\tIteration: 1600\t Loss: 0.7494\n",
      "\tIteration: 1640\t Loss: 0.7195\n",
      "\tIteration: 1680\t Loss: 0.6920\n",
      "\tIteration: 1720\t Loss: 0.6337\n",
      "\tIteration: 1760\t Loss: 0.6894\n",
      "\tIteration: 1800\t Loss: 0.6425\n",
      "\tIteration: 1840\t Loss: 0.6700\n",
      "\tIteration: 1880\t Loss: 0.6183\n",
      "\tIteration: 1920\t Loss: 0.5918\n",
      "\tIteration: 1960\t Loss: 0.6357\n",
      "\tIteration: 2000\t Loss: 0.6219\n",
      "\tIteration: 2040\t Loss: 0.5793\n",
      "\tIteration: 2080\t Loss: 0.5863\n",
      "\tIteration: 2120\t Loss: 0.5661\n",
      "\tIteration: 2160\t Loss: 0.5755\n",
      "\tIteration: 2200\t Loss: 0.5720\n",
      "\tIteration: 2240\t Loss: 0.5596\n",
      "\tIteration: 2280\t Loss: 0.4885\n",
      "\tIteration: 2320\t Loss: 0.5674\n",
      "\tIteration: 2360\t Loss: 0.5257\n",
      "\tIteration: 2400\t Loss: 0.5657\n",
      "\tIteration: 2440\t Loss: 0.5352\n",
      "\tIteration: 2480\t Loss: 0.4897\n",
      "\tIteration: 2520\t Loss: 0.5018\n",
      "\tIteration: 2560\t Loss: 0.4824\n",
      "\tIteration: 2600\t Loss: 0.4826\n",
      "\tIteration: 2640\t Loss: 0.4864\n",
      "\tIteration: 2680\t Loss: 0.5081\n",
      "\tIteration: 2720\t Loss: 0.4711\n",
      "\tIteration: 2760\t Loss: 0.4340\n",
      "\tIteration: 2800\t Loss: 0.4921\n",
      "\tIteration: 2840\t Loss: 0.4981\n",
      "\tIteration: 2880\t Loss: 0.4576\n",
      "\tIteration: 2920\t Loss: 0.5007\n",
      "\tIteration: 2960\t Loss: 0.4922\n",
      "\tIteration: 3000\t Loss: 0.4059\n",
      "\tIteration: 3040\t Loss: 0.4720\n",
      "\tIteration: 3080\t Loss: 0.3965\n",
      "\tIteration: 3120\t Loss: 0.4665\n",
      "\tIteration: 3160\t Loss: 0.4762\n",
      "\tIteration: 3200\t Loss: 0.4442\n",
      "\tIteration: 3240\t Loss: 0.4404\n",
      "\tIteration: 3280\t Loss: 0.4485\n",
      "\tIteration: 3320\t Loss: 0.4616\n",
      "\tIteration: 3360\t Loss: 0.5090\n",
      "\tIteration: 3400\t Loss: 0.4487\n",
      "\tIteration: 3440\t Loss: 0.4402\n",
      "\tIteration: 3480\t Loss: 0.4475\n",
      "\tIteration: 3520\t Loss: 0.4369\n",
      "\tIteration: 3560\t Loss: 0.4781\n",
      "\tIteration: 3600\t Loss: 0.4293\n",
      "\tIteration: 3640\t Loss: 0.3788\n",
      "\tIteration: 3680\t Loss: 0.4337\n",
      "\tIteration: 3720\t Loss: 0.3853\n",
      "Epoch: 2/3\n",
      "\tIteration: 0\t Loss: 0.0116\n",
      "\tIteration: 40\t Loss: 0.3656\n",
      "\tIteration: 80\t Loss: 0.4009\n",
      "\tIteration: 120\t Loss: 0.4198\n",
      "\tIteration: 160\t Loss: 0.3885\n",
      "\tIteration: 200\t Loss: 0.3667\n",
      "\tIteration: 240\t Loss: 0.4492\n",
      "\tIteration: 280\t Loss: 0.3992\n",
      "\tIteration: 320\t Loss: 0.4674\n",
      "\tIteration: 360\t Loss: 0.3281\n",
      "\tIteration: 400\t Loss: 0.3798\n",
      "\tIteration: 440\t Loss: 0.3196\n",
      "\tIteration: 480\t Loss: 0.3946\n",
      "\tIteration: 520\t Loss: 0.3571\n",
      "\tIteration: 560\t Loss: 0.3929\n",
      "\tIteration: 600\t Loss: 0.3962\n",
      "\tIteration: 640\t Loss: 0.4095\n",
      "\tIteration: 680\t Loss: 0.4173\n",
      "\tIteration: 720\t Loss: 0.3718\n",
      "\tIteration: 760\t Loss: 0.4414\n",
      "\tIteration: 800\t Loss: 0.3402\n",
      "\tIteration: 840\t Loss: 0.4529\n",
      "\tIteration: 880\t Loss: 0.4116\n",
      "\tIteration: 920\t Loss: 0.3959\n",
      "\tIteration: 960\t Loss: 0.3791\n",
      "\tIteration: 1000\t Loss: 0.3982\n",
      "\tIteration: 1040\t Loss: 0.3770\n",
      "\tIteration: 1080\t Loss: 0.3585\n",
      "\tIteration: 1120\t Loss: 0.3190\n",
      "\tIteration: 1160\t Loss: 0.4108\n",
      "\tIteration: 1200\t Loss: 0.3176\n",
      "\tIteration: 1240\t Loss: 0.3890\n",
      "\tIteration: 1280\t Loss: 0.3672\n",
      "\tIteration: 1320\t Loss: 0.3765\n",
      "\tIteration: 1360\t Loss: 0.3278\n",
      "\tIteration: 1400\t Loss: 0.3773\n",
      "\tIteration: 1440\t Loss: 0.3506\n",
      "\tIteration: 1480\t Loss: 0.4490\n",
      "\tIteration: 1520\t Loss: 0.3756\n",
      "\tIteration: 1560\t Loss: 0.4556\n",
      "\tIteration: 1600\t Loss: 0.3744\n",
      "\tIteration: 1640\t Loss: 0.3774\n",
      "\tIteration: 1680\t Loss: 0.3602\n",
      "\tIteration: 1720\t Loss: 0.3722\n",
      "\tIteration: 1760\t Loss: 0.3462\n",
      "\tIteration: 1800\t Loss: 0.3916\n",
      "\tIteration: 1840\t Loss: 0.3391\n",
      "\tIteration: 1880\t Loss: 0.3734\n",
      "\tIteration: 1920\t Loss: 0.4327\n",
      "\tIteration: 1960\t Loss: 0.3388\n",
      "\tIteration: 2000\t Loss: 0.3559\n",
      "\tIteration: 2040\t Loss: 0.3101\n",
      "\tIteration: 2080\t Loss: 0.3900\n",
      "\tIteration: 2120\t Loss: 0.3303\n",
      "\tIteration: 2160\t Loss: 0.3373\n",
      "\tIteration: 2200\t Loss: 0.3726\n",
      "\tIteration: 2240\t Loss: 0.3524\n",
      "\tIteration: 2280\t Loss: 0.3623\n",
      "\tIteration: 2320\t Loss: 0.3401\n",
      "\tIteration: 2360\t Loss: 0.3813\n",
      "\tIteration: 2400\t Loss: 0.3044\n",
      "\tIteration: 2440\t Loss: 0.3256\n",
      "\tIteration: 2480\t Loss: 0.2928\n",
      "\tIteration: 2520\t Loss: 0.3156\n",
      "\tIteration: 2560\t Loss: 0.3937\n",
      "\tIteration: 2600\t Loss: 0.2949\n",
      "\tIteration: 2640\t Loss: 0.3810\n",
      "\tIteration: 2680\t Loss: 0.3502\n",
      "\tIteration: 2720\t Loss: 0.3382\n",
      "\tIteration: 2760\t Loss: 0.3747\n",
      "\tIteration: 2800\t Loss: 0.3290\n",
      "\tIteration: 2840\t Loss: 0.2784\n",
      "\tIteration: 2880\t Loss: 0.3373\n",
      "\tIteration: 2920\t Loss: 0.3771\n",
      "\tIteration: 2960\t Loss: 0.3408\n",
      "\tIteration: 3000\t Loss: 0.3739\n",
      "\tIteration: 3040\t Loss: 0.3459\n",
      "\tIteration: 3080\t Loss: 0.3515\n",
      "\tIteration: 3120\t Loss: 0.3417\n",
      "\tIteration: 3160\t Loss: 0.3416\n",
      "\tIteration: 3200\t Loss: 0.3421\n",
      "\tIteration: 3240\t Loss: 0.3384\n",
      "\tIteration: 3280\t Loss: 0.3315\n",
      "\tIteration: 3320\t Loss: 0.3517\n",
      "\tIteration: 3360\t Loss: 0.3123\n",
      "\tIteration: 3400\t Loss: 0.2668\n",
      "\tIteration: 3440\t Loss: 0.3252\n",
      "\tIteration: 3480\t Loss: 0.3045\n",
      "\tIteration: 3520\t Loss: 0.3128\n",
      "\tIteration: 3560\t Loss: 0.3487\n",
      "\tIteration: 3600\t Loss: 0.2942\n",
      "\tIteration: 3640\t Loss: 0.3268\n",
      "\tIteration: 3680\t Loss: 0.3832\n",
      "\tIteration: 3720\t Loss: 0.3515\n",
      "Epoch: 3/3\n",
      "\tIteration: 0\t Loss: 0.0068\n",
      "\tIteration: 40\t Loss: 0.2565\n",
      "\tIteration: 80\t Loss: 0.3424\n",
      "\tIteration: 120\t Loss: 0.3726\n",
      "\tIteration: 160\t Loss: 0.3153\n",
      "\tIteration: 200\t Loss: 0.3579\n",
      "\tIteration: 240\t Loss: 0.2846\n",
      "\tIteration: 280\t Loss: 0.2940\n",
      "\tIteration: 320\t Loss: 0.3150\n",
      "\tIteration: 360\t Loss: 0.3236\n",
      "\tIteration: 400\t Loss: 0.2606\n",
      "\tIteration: 440\t Loss: 0.3572\n",
      "\tIteration: 480\t Loss: 0.3507\n",
      "\tIteration: 520\t Loss: 0.2812\n",
      "\tIteration: 560\t Loss: 0.3032\n",
      "\tIteration: 600\t Loss: 0.2681\n",
      "\tIteration: 640\t Loss: 0.3664\n",
      "\tIteration: 680\t Loss: 0.3142\n",
      "\tIteration: 720\t Loss: 0.2743\n",
      "\tIteration: 760\t Loss: 0.3071\n",
      "\tIteration: 800\t Loss: 0.2870\n",
      "\tIteration: 840\t Loss: 0.3198\n",
      "\tIteration: 880\t Loss: 0.3809\n",
      "\tIteration: 920\t Loss: 0.2672\n",
      "\tIteration: 960\t Loss: 0.2715\n",
      "\tIteration: 1000\t Loss: 0.3453\n",
      "\tIteration: 1040\t Loss: 0.3556\n",
      "\tIteration: 1080\t Loss: 0.3095\n",
      "\tIteration: 1120\t Loss: 0.3524\n",
      "\tIteration: 1160\t Loss: 0.2332\n",
      "\tIteration: 1200\t Loss: 0.4313\n",
      "\tIteration: 1240\t Loss: 0.2780\n",
      "\tIteration: 1280\t Loss: 0.4294\n",
      "\tIteration: 1320\t Loss: 0.3040\n",
      "\tIteration: 1360\t Loss: 0.2977\n",
      "\tIteration: 1400\t Loss: 0.2852\n",
      "\tIteration: 1440\t Loss: 0.3590\n",
      "\tIteration: 1480\t Loss: 0.2937\n",
      "\tIteration: 1520\t Loss: 0.3022\n",
      "\tIteration: 1560\t Loss: 0.2665\n",
      "\tIteration: 1600\t Loss: 0.2815\n",
      "\tIteration: 1640\t Loss: 0.2941\n",
      "\tIteration: 1680\t Loss: 0.3437\n",
      "\tIteration: 1720\t Loss: 0.2898\n",
      "\tIteration: 1760\t Loss: 0.2774\n",
      "\tIteration: 1800\t Loss: 0.3400\n",
      "\tIteration: 1840\t Loss: 0.2598\n",
      "\tIteration: 1880\t Loss: 0.3537\n",
      "\tIteration: 1920\t Loss: 0.3500\n",
      "\tIteration: 1960\t Loss: 0.2914\n",
      "\tIteration: 2000\t Loss: 0.2934\n",
      "\tIteration: 2040\t Loss: 0.3189\n",
      "\tIteration: 2080\t Loss: 0.3167\n",
      "\tIteration: 2120\t Loss: 0.2941\n",
      "\tIteration: 2160\t Loss: 0.3407\n",
      "\tIteration: 2200\t Loss: 0.3005\n",
      "\tIteration: 2240\t Loss: 0.2989\n",
      "\tIteration: 2280\t Loss: 0.2594\n",
      "\tIteration: 2320\t Loss: 0.3023\n",
      "\tIteration: 2360\t Loss: 0.3835\n",
      "\tIteration: 2400\t Loss: 0.2876\n",
      "\tIteration: 2440\t Loss: 0.2746\n",
      "\tIteration: 2480\t Loss: 0.2835\n",
      "\tIteration: 2520\t Loss: 0.2817\n",
      "\tIteration: 2560\t Loss: 0.3010\n",
      "\tIteration: 2600\t Loss: 0.3122\n",
      "\tIteration: 2640\t Loss: 0.3250\n",
      "\tIteration: 2680\t Loss: 0.3255\n",
      "\tIteration: 2720\t Loss: 0.3212\n",
      "\tIteration: 2760\t Loss: 0.2339\n",
      "\tIteration: 2800\t Loss: 0.3357\n",
      "\tIteration: 2840\t Loss: 0.3390\n",
      "\tIteration: 2880\t Loss: 0.2922\n",
      "\tIteration: 2920\t Loss: 0.2976\n",
      "\tIteration: 2960\t Loss: 0.3126\n",
      "\tIteration: 3000\t Loss: 0.3332\n",
      "\tIteration: 3040\t Loss: 0.3098\n",
      "\tIteration: 3080\t Loss: 0.2727\n",
      "\tIteration: 3120\t Loss: 0.2482\n",
      "\tIteration: 3160\t Loss: 0.2753\n",
      "\tIteration: 3200\t Loss: 0.3235\n",
      "\tIteration: 3240\t Loss: 0.3058\n",
      "\tIteration: 3280\t Loss: 0.3009\n",
      "\tIteration: 3320\t Loss: 0.2538\n",
      "\tIteration: 3360\t Loss: 0.2748\n",
      "\tIteration: 3400\t Loss: 0.2932\n",
      "\tIteration: 3440\t Loss: 0.3138\n",
      "\tIteration: 3480\t Loss: 0.2742\n",
      "\tIteration: 3520\t Loss: 0.2994\n",
      "\tIteration: 3560\t Loss: 0.2924\n",
      "\tIteration: 3600\t Loss: 0.2415\n",
      "\tIteration: 3640\t Loss: 0.2385\n",
      "\tIteration: 3680\t Loss: 0.2870\n",
      "\tIteration: 3720\t Loss: 0.2227\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "print_every = 40\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the network trained, we can check out it's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:30:00.206666Z",
     "start_time": "2021-05-26T22:29:59.954325Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x648 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 349,
       "height": 195
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = F.softmax(logits, dim=1)\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./model.pth\"\n",
    "torch.save(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our network is brilliant. It can accurately predict the digits in our images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\">EMNIST Classification: Exercise</h2>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 1:</h3>\n",
    "  <p>Now it's your turn to build a simple network, use any method I've covered so far. In the next notebook, you'll learn how to train a network so it can make good predictions.</p>\n",
    "  <p>Build a network to classify the MNIST images with 3 hidden layers. Use 16 units in the first hidden layer, 32 units in the second layer, and 8 units in the third layer. Each hidden layer should have a ReLU activation function, and use softmax on the output layer.</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Your network here\n",
    "class Strive_Network(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784,16)\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(32,8 )\n",
    "        self.fc4 = nn.Linear(8, 10)\n",
    "        \n",
    "    # Forward pass through the network, returns the output logits\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Strive_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "ps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x648 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 349,
       "height": 195
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Run this cell with your model to make sure it works\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 2:</h3>\n",
    "  <p>Train your network implementing the Pytorch training loop and <strong style=\"color:#01ff84\">after each epoch, use the model for predicting the test (validation) MNIST data.</strong></p>\n",
    "  <p>Note: If your model does not fit with the final softmax layer, you can remove this layer.</p>\n",
    "  <p>Hint: <a href=\"https://discuss.pytorch.org/t/training-loop-checking-validation-accuracy/78399\">Training loop checking validation accuracy\n",
    "</a></p>\n",
    "  <p>Research about <code>model.train()</code>, <code>model.eval()</code> and <code>with torch.no_grad()</code> in Pytorch.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/3\n",
      "\tIteration: 0\t Loss: 0.0575\n",
      "\tIteration: 40\t Loss: 2.3035\n",
      "\tIteration: 80\t Loss: 2.3005\n",
      "\tIteration: 120\t Loss: 2.3001\n",
      "\tIteration: 160\t Loss: 2.2964\n",
      "\tIteration: 200\t Loss: 2.2931\n",
      "\tIteration: 240\t Loss: 2.2810\n",
      "\tIteration: 280\t Loss: 2.2372\n",
      "\tIteration: 320\t Loss: 2.1672\n",
      "\tIteration: 360\t Loss: 2.0995\n",
      "\tIteration: 400\t Loss: 2.0879\n",
      "\tIteration: 440\t Loss: 2.0494\n",
      "\tIteration: 480\t Loss: 2.0163\n",
      "\tIteration: 520\t Loss: 2.0171\n",
      "\tIteration: 560\t Loss: 1.9722\n",
      "\tIteration: 600\t Loss: 1.9637\n",
      "\tIteration: 640\t Loss: 1.9810\n",
      "\tIteration: 680\t Loss: 1.9309\n",
      "\tIteration: 720\t Loss: 1.8572\n",
      "\tIteration: 760\t Loss: 1.8959\n",
      "\tIteration: 800\t Loss: 1.8630\n",
      "\tIteration: 840\t Loss: 1.8492\n",
      "\tIteration: 880\t Loss: 1.8493\n",
      "\tIteration: 920\t Loss: 1.8823\n",
      "\tIteration: 960\t Loss: 1.8766\n",
      "\tIteration: 1000\t Loss: 1.8688\n",
      "\tIteration: 1040\t Loss: 1.8771\n",
      "\tIteration: 1080\t Loss: 1.8496\n",
      "\tIteration: 1120\t Loss: 1.8150\n",
      "\tIteration: 1160\t Loss: 1.8276\n",
      "\tIteration: 1200\t Loss: 1.8145\n",
      "\tIteration: 1240\t Loss: 1.8656\n",
      "\tIteration: 1280\t Loss: 1.8299\n",
      "\tIteration: 1320\t Loss: 1.8461\n",
      "\tIteration: 1360\t Loss: 1.8301\n",
      "\tIteration: 1400\t Loss: 1.8083\n",
      "\tIteration: 1440\t Loss: 1.8651\n",
      "\tIteration: 1480\t Loss: 1.8282\n",
      "\tIteration: 1520\t Loss: 1.8397\n",
      "\tIteration: 1560\t Loss: 1.8363\n",
      "\tIteration: 1600\t Loss: 1.8275\n",
      "\tIteration: 1640\t Loss: 1.8317\n",
      "\tIteration: 1680\t Loss: 1.8337\n",
      "\tIteration: 1720\t Loss: 1.8157\n",
      "\tIteration: 1760\t Loss: 1.8003\n",
      "\tIteration: 1800\t Loss: 1.8286\n",
      "\tIteration: 1840\t Loss: 1.8376\n",
      "\tIteration: 1880\t Loss: 1.8389\n",
      "\tIteration: 1920\t Loss: 1.8096\n",
      "\tIteration: 1960\t Loss: 1.8326\n",
      "\tIteration: 2000\t Loss: 1.8066\n",
      "\tIteration: 2040\t Loss: 1.8409\n",
      "\tIteration: 2080\t Loss: 1.8252\n",
      "\tIteration: 2120\t Loss: 1.7902\n",
      "\tIteration: 2160\t Loss: 1.8346\n",
      "\tIteration: 2200\t Loss: 1.8301\n",
      "\tIteration: 2240\t Loss: 1.8177\n",
      "\tIteration: 2280\t Loss: 1.7881\n",
      "\tIteration: 2320\t Loss: 1.8188\n",
      "\tIteration: 2360\t Loss: 1.8133\n",
      "\tIteration: 2400\t Loss: 1.7952\n",
      "\tIteration: 2440\t Loss: 1.8220\n",
      "\tIteration: 2480\t Loss: 1.8015\n",
      "\tIteration: 2520\t Loss: 1.8249\n",
      "\tIteration: 2560\t Loss: 1.7948\n",
      "\tIteration: 2600\t Loss: 1.8023\n",
      "\tIteration: 2640\t Loss: 1.8290\n",
      "\tIteration: 2680\t Loss: 1.8014\n",
      "\tIteration: 2720\t Loss: 1.8118\n",
      "\tIteration: 2760\t Loss: 1.7785\n",
      "\tIteration: 2800\t Loss: 1.8088\n",
      "\tIteration: 2840\t Loss: 1.7964\n",
      "\tIteration: 2880\t Loss: 1.8363\n",
      "\tIteration: 2920\t Loss: 1.8189\n",
      "\tIteration: 2960\t Loss: 1.8046\n",
      "\tIteration: 3000\t Loss: 1.7813\n",
      "\tIteration: 3040\t Loss: 1.7786\n",
      "\tIteration: 3080\t Loss: 1.7844\n",
      "\tIteration: 3120\t Loss: 1.7964\n",
      "\tIteration: 3160\t Loss: 1.7406\n",
      "\tIteration: 3200\t Loss: 1.7715\n",
      "\tIteration: 3240\t Loss: 1.7507\n",
      "\tIteration: 3280\t Loss: 1.7464\n",
      "\tIteration: 3320\t Loss: 1.7253\n",
      "\tIteration: 3360\t Loss: 1.7374\n",
      "\tIteration: 3400\t Loss: 1.7482\n",
      "\tIteration: 3440\t Loss: 1.7635\n",
      "\tIteration: 3480\t Loss: 1.7332\n",
      "\tIteration: 3520\t Loss: 1.7447\n",
      "\tIteration: 3560\t Loss: 1.7342\n",
      "\tIteration: 3600\t Loss: 1.7420\n",
      "\tIteration: 3640\t Loss: 1.7042\n",
      "\tIteration: 3680\t Loss: 1.7318\n",
      "\tIteration: 3720\t Loss: 1.7427\n",
      "Accuracy of the network: 73 %\n",
      "Epoch: 2/3\n",
      "\tIteration: 0\t Loss: 0.0427\n",
      "\tIteration: 40\t Loss: 1.7791\n",
      "\tIteration: 80\t Loss: 1.7518\n",
      "\tIteration: 120\t Loss: 1.7231\n",
      "\tIteration: 160\t Loss: 1.7270\n",
      "\tIteration: 200\t Loss: 1.7277\n",
      "\tIteration: 240\t Loss: 1.7319\n",
      "\tIteration: 280\t Loss: 1.7395\n",
      "\tIteration: 320\t Loss: 1.7256\n",
      "\tIteration: 360\t Loss: 1.7398\n",
      "\tIteration: 400\t Loss: 1.7342\n",
      "\tIteration: 440\t Loss: 1.7395\n",
      "\tIteration: 480\t Loss: 1.7066\n",
      "\tIteration: 520\t Loss: 1.7115\n",
      "\tIteration: 560\t Loss: 1.7164\n",
      "\tIteration: 600\t Loss: 1.7189\n",
      "\tIteration: 640\t Loss: 1.7323\n",
      "\tIteration: 680\t Loss: 1.7117\n",
      "\tIteration: 720\t Loss: 1.7294\n",
      "\tIteration: 760\t Loss: 1.7168\n",
      "\tIteration: 800\t Loss: 1.7214\n",
      "\tIteration: 840\t Loss: 1.7010\n",
      "\tIteration: 880\t Loss: 1.7313\n",
      "\tIteration: 920\t Loss: 1.7386\n",
      "\tIteration: 960\t Loss: 1.7140\n",
      "\tIteration: 1000\t Loss: 1.7360\n",
      "\tIteration: 1040\t Loss: 1.7393\n",
      "\tIteration: 1080\t Loss: 1.7488\n",
      "\tIteration: 1120\t Loss: 1.7133\n",
      "\tIteration: 1160\t Loss: 1.7758\n",
      "\tIteration: 1200\t Loss: 1.7487\n",
      "\tIteration: 1240\t Loss: 1.7321\n",
      "\tIteration: 1280\t Loss: 1.7020\n",
      "\tIteration: 1320\t Loss: 1.7418\n",
      "\tIteration: 1360\t Loss: 1.7217\n",
      "\tIteration: 1400\t Loss: 1.7561\n",
      "\tIteration: 1440\t Loss: 1.7240\n",
      "\tIteration: 1480\t Loss: 1.7682\n",
      "\tIteration: 1520\t Loss: 1.7310\n",
      "\tIteration: 1560\t Loss: 1.7346\n",
      "\tIteration: 1600\t Loss: 1.7258\n",
      "\tIteration: 1640\t Loss: 1.7482\n",
      "\tIteration: 1680\t Loss: 1.7325\n",
      "\tIteration: 1720\t Loss: 1.7478\n",
      "\tIteration: 1760\t Loss: 1.7218\n",
      "\tIteration: 1800\t Loss: 1.7109\n",
      "\tIteration: 1840\t Loss: 1.7492\n",
      "\tIteration: 1880\t Loss: 1.7114\n",
      "\tIteration: 1920\t Loss: 1.7309\n",
      "\tIteration: 1960\t Loss: 1.7502\n",
      "\tIteration: 2000\t Loss: 1.7178\n",
      "\tIteration: 2040\t Loss: 1.7129\n",
      "\tIteration: 2080\t Loss: 1.7214\n",
      "\tIteration: 2120\t Loss: 1.6956\n",
      "\tIteration: 2160\t Loss: 1.7319\n",
      "\tIteration: 2200\t Loss: 1.6993\n",
      "\tIteration: 2240\t Loss: 1.7215\n",
      "\tIteration: 2280\t Loss: 1.7237\n",
      "\tIteration: 2320\t Loss: 1.7324\n",
      "\tIteration: 2360\t Loss: 1.7085\n",
      "\tIteration: 2400\t Loss: 1.7136\n",
      "\tIteration: 2440\t Loss: 1.7379\n",
      "\tIteration: 2480\t Loss: 1.7461\n",
      "\tIteration: 2520\t Loss: 1.7334\n",
      "\tIteration: 2560\t Loss: 1.7068\n",
      "\tIteration: 2600\t Loss: 1.7389\n",
      "\tIteration: 2640\t Loss: 1.7091\n",
      "\tIteration: 2680\t Loss: 1.7094\n",
      "\tIteration: 2720\t Loss: 1.7288\n",
      "\tIteration: 2760\t Loss: 1.7231\n",
      "\tIteration: 2800\t Loss: 1.7149\n",
      "\tIteration: 2840\t Loss: 1.7097\n",
      "\tIteration: 2880\t Loss: 1.6709\n",
      "\tIteration: 2920\t Loss: 1.7427\n",
      "\tIteration: 2960\t Loss: 1.7029\n",
      "\tIteration: 3000\t Loss: 1.6908\n",
      "\tIteration: 3040\t Loss: 1.6874\n",
      "\tIteration: 3080\t Loss: 1.7057\n",
      "\tIteration: 3120\t Loss: 1.6851\n",
      "\tIteration: 3160\t Loss: 1.6829\n",
      "\tIteration: 3200\t Loss: 1.6950\n",
      "\tIteration: 3240\t Loss: 1.6874\n",
      "\tIteration: 3280\t Loss: 1.6732\n",
      "\tIteration: 3320\t Loss: 1.7000\n",
      "\tIteration: 3360\t Loss: 1.6943\n",
      "\tIteration: 3400\t Loss: 1.6759\n",
      "\tIteration: 3440\t Loss: 1.6558\n",
      "\tIteration: 3480\t Loss: 1.7054\n",
      "\tIteration: 3520\t Loss: 1.7017\n",
      "\tIteration: 3560\t Loss: 1.6879\n",
      "\tIteration: 3600\t Loss: 1.7150\n",
      "\tIteration: 3640\t Loss: 1.6766\n",
      "\tIteration: 3680\t Loss: 1.6826\n",
      "\tIteration: 3720\t Loss: 1.6959\n",
      "Accuracy of the network: 80 %\n",
      "Epoch: 3/3\n",
      "\tIteration: 0\t Loss: 0.0430\n",
      "\tIteration: 40\t Loss: 1.6554\n",
      "\tIteration: 80\t Loss: 1.6470\n",
      "\tIteration: 120\t Loss: 1.7155\n",
      "\tIteration: 160\t Loss: 1.6855\n",
      "\tIteration: 200\t Loss: 1.6914\n",
      "\tIteration: 240\t Loss: 1.6986\n",
      "\tIteration: 280\t Loss: 1.6951\n",
      "\tIteration: 320\t Loss: 1.6985\n",
      "\tIteration: 360\t Loss: 1.6570\n",
      "\tIteration: 400\t Loss: 1.6895\n",
      "\tIteration: 440\t Loss: 1.7211\n",
      "\tIteration: 480\t Loss: 1.7016\n",
      "\tIteration: 520\t Loss: 1.6780\n",
      "\tIteration: 560\t Loss: 1.6695\n",
      "\tIteration: 600\t Loss: 1.6695\n",
      "\tIteration: 640\t Loss: 1.6952\n",
      "\tIteration: 680\t Loss: 1.6462\n",
      "\tIteration: 720\t Loss: 1.6729\n",
      "\tIteration: 760\t Loss: 1.6740\n",
      "\tIteration: 800\t Loss: 1.6762\n",
      "\tIteration: 840\t Loss: 1.6838\n",
      "\tIteration: 880\t Loss: 1.6699\n",
      "\tIteration: 920\t Loss: 1.6826\n",
      "\tIteration: 960\t Loss: 1.6489\n",
      "\tIteration: 1000\t Loss: 1.7207\n",
      "\tIteration: 1040\t Loss: 1.6939\n",
      "\tIteration: 1080\t Loss: 1.6792\n",
      "\tIteration: 1120\t Loss: 1.6673\n",
      "\tIteration: 1160\t Loss: 1.6704\n",
      "\tIteration: 1200\t Loss: 1.7012\n",
      "\tIteration: 1240\t Loss: 1.7130\n",
      "\tIteration: 1280\t Loss: 1.6547\n",
      "\tIteration: 1320\t Loss: 1.6859\n",
      "\tIteration: 1360\t Loss: 1.6595\n",
      "\tIteration: 1400\t Loss: 1.6726\n",
      "\tIteration: 1440\t Loss: 1.6724\n",
      "\tIteration: 1480\t Loss: 1.6708\n",
      "\tIteration: 1520\t Loss: 1.6704\n",
      "\tIteration: 1560\t Loss: 1.6504\n",
      "\tIteration: 1600\t Loss: 1.6438\n",
      "\tIteration: 1640\t Loss: 1.6991\n",
      "\tIteration: 1680\t Loss: 1.6914\n",
      "\tIteration: 1720\t Loss: 1.6698\n",
      "\tIteration: 1760\t Loss: 1.7193\n",
      "\tIteration: 1800\t Loss: 1.6558\n",
      "\tIteration: 1840\t Loss: 1.6628\n",
      "\tIteration: 1880\t Loss: 1.6975\n",
      "\tIteration: 1920\t Loss: 1.6629\n",
      "\tIteration: 1960\t Loss: 1.6622\n",
      "\tIteration: 2000\t Loss: 1.6434\n",
      "\tIteration: 2040\t Loss: 1.6694\n",
      "\tIteration: 2080\t Loss: 1.6391\n",
      "\tIteration: 2120\t Loss: 1.6452\n",
      "\tIteration: 2160\t Loss: 1.6728\n",
      "\tIteration: 2200\t Loss: 1.6713\n",
      "\tIteration: 2240\t Loss: 1.6472\n",
      "\tIteration: 2280\t Loss: 1.6636\n",
      "\tIteration: 2320\t Loss: 1.6694\n",
      "\tIteration: 2360\t Loss: 1.6675\n",
      "\tIteration: 2400\t Loss: 1.6329\n",
      "\tIteration: 2440\t Loss: 1.6520\n",
      "\tIteration: 2480\t Loss: 1.6847\n",
      "\tIteration: 2520\t Loss: 1.6503\n",
      "\tIteration: 2560\t Loss: 1.6605\n",
      "\tIteration: 2600\t Loss: 1.6959\n",
      "\tIteration: 2640\t Loss: 1.6506\n",
      "\tIteration: 2680\t Loss: 1.6557\n",
      "\tIteration: 2720\t Loss: 1.6334\n",
      "\tIteration: 2760\t Loss: 1.6836\n",
      "\tIteration: 2800\t Loss: 1.6674\n",
      "\tIteration: 2840\t Loss: 1.6831\n",
      "\tIteration: 2880\t Loss: 1.6652\n",
      "\tIteration: 2920\t Loss: 1.6658\n",
      "\tIteration: 2960\t Loss: 1.6681\n",
      "\tIteration: 3000\t Loss: 1.6627\n",
      "\tIteration: 3040\t Loss: 1.6846\n",
      "\tIteration: 3080\t Loss: 1.6769\n",
      "\tIteration: 3120\t Loss: 1.6571\n",
      "\tIteration: 3160\t Loss: 1.7126\n",
      "\tIteration: 3200\t Loss: 1.6289\n",
      "\tIteration: 3240\t Loss: 1.6507\n",
      "\tIteration: 3280\t Loss: 1.6472\n",
      "\tIteration: 3320\t Loss: 1.6679\n",
      "\tIteration: 3360\t Loss: 1.6575\n",
      "\tIteration: 3400\t Loss: 1.6323\n",
      "\tIteration: 3440\t Loss: 1.6520\n",
      "\tIteration: 3480\t Loss: 1.6467\n",
      "\tIteration: 3520\t Loss: 1.6582\n",
      "\tIteration: 3560\t Loss: 1.6439\n",
      "\tIteration: 3600\t Loss: 1.6628\n",
      "\tIteration: 3640\t Loss: 1.6943\n",
      "\tIteration: 3680\t Loss: 1.6764\n",
      "\tIteration: 3720\t Loss: 1.6598\n",
      "Accuracy of the network: 78 %\n"
     ]
    }
   ],
   "source": [
    "## TODO: Your training loop here\n",
    "epochs = 3\n",
    "print_every = 40\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(iter(testloader)):\n",
    "            images.resize_(images.size()[0], 784)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'testloader' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c2c0e89f1b1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run this cell with your model to make sure it works and predicts well for the validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Run this cell with your model to make sure it works and predicts well for the validation data\n",
    "images, labels = next(iter(testloader))\n",
    "images.resize_(images.shape[0],1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "ps.shape\n",
    "#view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 3:</h3>\n",
    "  <p>Write the code for adding <strong style=\"color:#01ff84\">Early Stopping with patience = 2</strong> to the training loop from scratch.</p>\n",
    "  <p><strong style=\"color:#01ff84\">Hint:</strong> Monitor the Validation loss every epoch, and if in 2 epochs, the validation loss does not improve, stop the training loop with <code>break</code>.</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/5\n",
      "\tIteration: 0\t Loss: 0.0412\n",
      "\tIteration: 40\t Loss: 1.5941\n",
      "\tIteration: 80\t Loss: 1.5747\n",
      "\tIteration: 120\t Loss: 1.5624\n",
      "\tIteration: 160\t Loss: 1.5691\n",
      "\tIteration: 200\t Loss: 1.5825\n",
      "\tIteration: 240\t Loss: 1.6226\n",
      "\tIteration: 280\t Loss: 1.6011\n",
      "\tIteration: 320\t Loss: 1.5766\n",
      "\tIteration: 360\t Loss: 1.5690\n",
      "\tIteration: 400\t Loss: 1.6596\n",
      "\tIteration: 440\t Loss: 1.6236\n",
      "\tIteration: 480\t Loss: 1.5961\n",
      "\tIteration: 520\t Loss: 1.5918\n",
      "\tIteration: 560\t Loss: 1.6314\n",
      "\tIteration: 600\t Loss: 1.5954\n",
      "\tIteration: 640\t Loss: 1.5536\n",
      "\tIteration: 680\t Loss: 1.5903\n",
      "\tIteration: 720\t Loss: 1.6133\n",
      "\tIteration: 760\t Loss: 1.5953\n",
      "\tIteration: 800\t Loss: 1.5702\n",
      "\tIteration: 840\t Loss: 1.5743\n",
      "\tIteration: 880\t Loss: 1.5953\n",
      "\tIteration: 920\t Loss: 1.5875\n",
      "\tIteration: 960\t Loss: 1.6221\n",
      "\tIteration: 1000\t Loss: 1.5977\n",
      "\tIteration: 1040\t Loss: 1.6057\n",
      "\tIteration: 1080\t Loss: 1.5884\n",
      "\tIteration: 1120\t Loss: 1.5746\n",
      "\tIteration: 1160\t Loss: 1.5808\n",
      "\tIteration: 1200\t Loss: 1.5804\n",
      "\tIteration: 1240\t Loss: 1.6025\n",
      "\tIteration: 1280\t Loss: 1.6148\n",
      "\tIteration: 1320\t Loss: 1.5949\n",
      "\tIteration: 1360\t Loss: 1.5934\n",
      "\tIteration: 1400\t Loss: 1.5736\n",
      "\tIteration: 1440\t Loss: 1.6079\n",
      "\tIteration: 1480\t Loss: 1.5694\n",
      "\tIteration: 1520\t Loss: 1.5805\n",
      "\tIteration: 1560\t Loss: 1.6057\n",
      "\tIteration: 1600\t Loss: 1.5987\n",
      "\tIteration: 1640\t Loss: 1.5958\n",
      "\tIteration: 1680\t Loss: 1.5845\n",
      "\tIteration: 1720\t Loss: 1.5566\n",
      "\tIteration: 1760\t Loss: 1.5732\n",
      "\tIteration: 1800\t Loss: 1.6078\n",
      "\tIteration: 1840\t Loss: 1.6257\n",
      "\tIteration: 1880\t Loss: 1.6331\n",
      "\tIteration: 1920\t Loss: 1.5847\n",
      "\tIteration: 1960\t Loss: 1.5661\n",
      "\tIteration: 2000\t Loss: 1.6072\n",
      "\tIteration: 2040\t Loss: 1.5932\n",
      "\tIteration: 2080\t Loss: 1.6022\n",
      "\tIteration: 2120\t Loss: 1.5462\n",
      "\tIteration: 2160\t Loss: 1.5643\n",
      "\tIteration: 2200\t Loss: 1.5660\n",
      "\tIteration: 2240\t Loss: 1.5816\n",
      "\tIteration: 2280\t Loss: 1.5988\n",
      "\tIteration: 2320\t Loss: 1.5953\n",
      "\tIteration: 2360\t Loss: 1.6101\n",
      "\tIteration: 2400\t Loss: 1.5911\n",
      "\tIteration: 2440\t Loss: 1.6391\n",
      "\tIteration: 2480\t Loss: 1.6538\n",
      "\tIteration: 2520\t Loss: 1.5871\n",
      "\tIteration: 2560\t Loss: 1.5941\n",
      "\tIteration: 2600\t Loss: 1.5724\n",
      "\tIteration: 2640\t Loss: 1.6069\n",
      "\tIteration: 2680\t Loss: 1.5962\n",
      "\tIteration: 2720\t Loss: 1.5973\n",
      "\tIteration: 2760\t Loss: 1.5957\n",
      "\tIteration: 2800\t Loss: 1.6053\n",
      "\tIteration: 2840\t Loss: 1.6674\n",
      "\tIteration: 2880\t Loss: 1.6043\n",
      "\tIteration: 2920\t Loss: 1.5756\n",
      "\tIteration: 2960\t Loss: 1.6153\n",
      "\tIteration: 3000\t Loss: 1.5920\n",
      "\tIteration: 3040\t Loss: 1.6890\n",
      "\tIteration: 3080\t Loss: 1.5582\n",
      "\tIteration: 3120\t Loss: 1.6133\n",
      "\tIteration: 3160\t Loss: 1.5699\n",
      "\tIteration: 3200\t Loss: 1.6134\n",
      "\tIteration: 3240\t Loss: 1.5597\n",
      "\tIteration: 3280\t Loss: 1.6354\n",
      "\tIteration: 3320\t Loss: 1.6024\n",
      "\tIteration: 3360\t Loss: 1.6186\n",
      "\tIteration: 3400\t Loss: 1.6085\n",
      "\tIteration: 3440\t Loss: 1.6180\n",
      "\tIteration: 3480\t Loss: 1.5941\n",
      "\tIteration: 3520\t Loss: 1.5928\n",
      "\tIteration: 3560\t Loss: 1.5716\n",
      "\tIteration: 3600\t Loss: 1.6150\n",
      "\tIteration: 3640\t Loss: 1.5921\n",
      "\tIteration: 3680\t Loss: 1.6220\n",
      "\tIteration: 3720\t Loss: 1.5694\n",
      "Accuracy of the network: 87 %\n",
      "Validation loss: 988.8927297592163\n",
      "Epoch: 2/5\n",
      "\tIteration: 0\t Loss: 0.0367\n",
      "\tIteration: 40\t Loss: 1.5748\n",
      "\tIteration: 80\t Loss: 1.5797\n",
      "\tIteration: 120\t Loss: 1.5886\n",
      "\tIteration: 160\t Loss: 1.6031\n",
      "\tIteration: 200\t Loss: 1.5974\n",
      "\tIteration: 240\t Loss: 1.5697\n",
      "\tIteration: 280\t Loss: 1.6190\n",
      "\tIteration: 320\t Loss: 1.6156\n",
      "\tIteration: 360\t Loss: 1.5871\n",
      "\tIteration: 400\t Loss: 1.6306\n",
      "\tIteration: 440\t Loss: 1.5816\n",
      "\tIteration: 480\t Loss: 1.5636\n",
      "\tIteration: 520\t Loss: 1.6367\n",
      "\tIteration: 560\t Loss: 1.6249\n",
      "\tIteration: 600\t Loss: 1.6109\n",
      "\tIteration: 640\t Loss: 1.6096\n",
      "\tIteration: 680\t Loss: 1.6363\n",
      "\tIteration: 720\t Loss: 1.6145\n",
      "\tIteration: 760\t Loss: 1.6427\n",
      "\tIteration: 800\t Loss: 1.6672\n",
      "\tIteration: 840\t Loss: 1.7075\n",
      "\tIteration: 880\t Loss: 1.6610\n",
      "\tIteration: 920\t Loss: 1.6476\n",
      "\tIteration: 960\t Loss: 1.5980\n",
      "\tIteration: 1000\t Loss: 1.6039\n",
      "\tIteration: 1040\t Loss: 1.5911\n",
      "\tIteration: 1080\t Loss: 1.5898\n",
      "\tIteration: 1120\t Loss: 1.6071\n",
      "\tIteration: 1160\t Loss: 1.5775\n",
      "\tIteration: 1200\t Loss: 1.6490\n",
      "\tIteration: 1240\t Loss: 1.6130\n",
      "\tIteration: 1280\t Loss: 1.6401\n",
      "\tIteration: 1320\t Loss: 1.6931\n",
      "\tIteration: 1360\t Loss: 1.5703\n",
      "\tIteration: 1400\t Loss: 1.5933\n",
      "\tIteration: 1440\t Loss: 1.6221\n",
      "\tIteration: 1480\t Loss: 1.5857\n",
      "\tIteration: 1520\t Loss: 1.6078\n",
      "\tIteration: 1560\t Loss: 1.6608\n",
      "\tIteration: 1600\t Loss: 1.6470\n",
      "\tIteration: 1640\t Loss: 1.5783\n",
      "\tIteration: 1680\t Loss: 1.5980\n",
      "\tIteration: 1720\t Loss: 1.6372\n",
      "\tIteration: 1760\t Loss: 1.6526\n",
      "\tIteration: 1800\t Loss: 1.6311\n",
      "\tIteration: 1840\t Loss: 1.6103\n",
      "\tIteration: 1880\t Loss: 1.6003\n",
      "\tIteration: 1920\t Loss: 1.6278\n",
      "\tIteration: 1960\t Loss: 1.6270\n",
      "\tIteration: 2000\t Loss: 1.6225\n",
      "\tIteration: 2040\t Loss: 1.6720\n",
      "\tIteration: 2080\t Loss: 1.5733\n",
      "\tIteration: 2120\t Loss: 1.5972\n",
      "\tIteration: 2160\t Loss: 1.5818\n",
      "\tIteration: 2200\t Loss: 1.5624\n",
      "\tIteration: 2240\t Loss: 1.6156\n",
      "\tIteration: 2280\t Loss: 1.5931\n",
      "\tIteration: 2320\t Loss: 1.6422\n",
      "\tIteration: 2360\t Loss: 1.6104\n",
      "\tIteration: 2400\t Loss: 1.6075\n",
      "\tIteration: 2440\t Loss: 1.6050\n",
      "\tIteration: 2480\t Loss: 1.5911\n",
      "\tIteration: 2520\t Loss: 1.5754\n",
      "\tIteration: 2560\t Loss: 1.5760\n",
      "\tIteration: 2600\t Loss: 1.6751\n",
      "\tIteration: 2640\t Loss: 1.7044\n",
      "\tIteration: 2680\t Loss: 1.6434\n",
      "\tIteration: 2720\t Loss: 1.6192\n",
      "\tIteration: 2760\t Loss: 1.6549\n",
      "\tIteration: 2800\t Loss: 1.6363\n",
      "\tIteration: 2840\t Loss: 1.6240\n",
      "\tIteration: 2880\t Loss: 1.6134\n",
      "\tIteration: 2920\t Loss: 1.6410\n",
      "\tIteration: 2960\t Loss: 1.7225\n",
      "\tIteration: 3000\t Loss: 1.6204\n",
      "\tIteration: 3040\t Loss: 1.5895\n",
      "\tIteration: 3080\t Loss: 1.5845\n",
      "\tIteration: 3120\t Loss: 1.5711\n",
      "\tIteration: 3160\t Loss: 1.5803\n",
      "\tIteration: 3200\t Loss: 1.6121\n",
      "\tIteration: 3240\t Loss: 1.6240\n",
      "\tIteration: 3280\t Loss: 1.5782\n",
      "\tIteration: 3320\t Loss: 1.6207\n",
      "\tIteration: 3360\t Loss: 1.6380\n",
      "\tIteration: 3400\t Loss: 1.6028\n",
      "\tIteration: 3440\t Loss: 1.6636\n",
      "\tIteration: 3480\t Loss: 1.5869\n",
      "\tIteration: 3520\t Loss: 1.6370\n",
      "\tIteration: 3560\t Loss: 1.7316\n",
      "\tIteration: 3600\t Loss: 1.6384\n",
      "\tIteration: 3640\t Loss: 1.5994\n",
      "\tIteration: 3680\t Loss: 1.6071\n",
      "\tIteration: 3720\t Loss: 1.6077\n",
      "Accuracy of the network: 86 %\n",
      "Validation loss: 998.0568175315857\n"
     ]
    }
   ],
   "source": [
    "## TODO: Your training loop here\n",
    "best_validation_loss = np.inf\n",
    "epochs = 5\n",
    "print_every = 40\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    running_validation_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(iter(testloader)):\n",
    "            images.resize_(images.size()[0], 784)\n",
    "            output = model(images)\n",
    "            running_validation_loss += criterion(output, labels).item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network: %d %%' % (\n",
    "        100 * correct / total))\n",
    "    print(f'Validation loss: {running_validation_loss}')\n",
    "    if best_validation_loss > running_validation_loss:\n",
    "        best_validation_loss = running_validation_loss\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Optional:</h3>\n",
    "  <p>Don't you want to use MNIST? Try EMNIST instead! Maybe using the first 10 letters of the alphabet!</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:35:26.981584Z",
     "start_time": "2021-05-26T22:35:26.954522Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will need a custom visualization function\n",
    "def view_classify_emnist(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(list(\"abcdefghij\"), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:50:57.571260Z",
     "start_time": "2021-05-26T22:50:57.322172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data (Preprocessing)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5)) ])\n",
    "def my_collate(batch):\n",
    "    modified_batch = []\n",
    "    for item in batch:\n",
    "        image, label = item\n",
    "        if label < 10: # only the first ten letters\n",
    "            modified_batch.append(item)\n",
    "    return torch.utils.data._utils.collate.default_collate(modified_batch)\n",
    "\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.EMNIST('EMNIST_data/', split=\"letters\", download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, collate_fn=my_collate)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.EMNIST('EMNIST_data/', split=\"letters\", download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=True, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:51:02.493175Z",
     "start_time": "2021-05-26T22:51:02.464301Z"
    }
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:51:03.118421Z",
     "start_time": "2021-05-26T22:51:02.978678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[5].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:51:06.653639Z",
     "start_time": "2021-05-26T22:51:06.647991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 9, 6, 7, 5, 1, 9, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd050a93f2fecfd00da27f8930a2c1c74e92db967b2162384b3e8848f4306dc0d4b",
   "display_name": "Python 3.7.10 64-bit ('deeplearner': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "metadata": {
   "interpreter": {
    "hash": "50a93f2fecfd00da27f8930a2c1c74e92db967b2162384b3e8848f4306dc0d4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}