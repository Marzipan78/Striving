{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\">Deep Learning Fundamentals</h2>\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\">Multiclass Classification: MNIST</h2>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:36.081105Z",
     "start_time": "2021-05-26T22:26:35.040138Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxliary plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:37.473177Z",
     "start_time": "2021-05-26T22:26:37.465910Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/view-classify-in-module-helper/30279/6\n",
    "\n",
    "def view_classify(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Dataset\n",
    "First up, we need to get our dataset. This is provided through the `torchvision` package. The code below will download the MNIST dataset, then create training and test datasets for us. Don't worry too much about the details here, you'll learn more about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:38.402766Z",
     "start_time": "2021-05-26T22:26:38.298968Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "100.1%Extracting MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "113.5%Extracting MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "100.4%Extracting MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "180.4%Extracting MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "Processing...\n",
      "Done!\n",
      "C:\\Users\\pgsin\\anaconda3\\envs\\deeplearner\\lib\\site-packages\\torchvision\\datasets\\mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Define a transform to normalize the data (Preprocessing)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5)) ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:38.632988Z",
     "start_time": "2021-05-26T22:26:38.477558Z"
    }
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. We'd use this to loop through the dataset for training, but here I'm just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size (64, 1, 28, 28). So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:39.407000Z",
     "start_time": "2021-05-26T22:26:39.265256Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 251,
       "height": 248
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch\n",
    "\n",
    "Here I'll use PyTorch to build a simple feedfoward network to classify the MNIST images. That is, the network will receive a digit image as input and predict the digit in the image.\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width=600px>\n",
    "\n",
    "To build a neural network with PyTorch, you use the `torch.nn` module. The network itself is a class inheriting from `torch.nn.Module`. You define each of the operations separately, like `nn.Linear(784, 128)` for a fully connected linear layer with 784 inputs and 128 units.\n",
    "\n",
    "The class needs to include a `forward` method that implements the forward pass through the network. In this method, you pass some input tensor `x` through each of the operations you defined earlier. The `torch.nn` module also has functional equivalents for things like ReLUs in `torch.nn.functional`. This module is usually imported as `F`. Then to use a ReLU activation on some layer (which is just a tensor), you'd do `F.relu(x)`. Below are a few different commonly used activation functions.\n",
    "\n",
    "<img src=\"assets/activation.png\" width=700px>\n",
    "\n",
    "So, for this network, I'll build it with three fully connected layers, then a softmax output for predicting classes. The softmax function is similar to the sigmoid in that it squashes inputs between 0 and 1, but it's also normalized so that all the values sum to one like a proper probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:39.961531Z",
     "start_time": "2021-05-26T22:26:39.946776Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    # Defining the layers, 128, 64, 10 units each\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 10)\n",
    "        \n",
    "    # Forward pass through the network, returns the output logits\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the input features are 784? Because the input images have size 28 pixels x 28 pixels for a total of 784 features. Since a Multilayer perceptron accepts only flatten inputs, we need to flatten a 28x28 grid into a 784 array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential API\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:41.213448Z",
     "start_time": "2021-05-26T22:26:41.205216Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Linear(in_features=784, out_features=4, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=4, out_features=4, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=4, out_features=10, bias=True)\n  (5): Softmax(dim=1)\n)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [4, 4]\n",
    "output_size   = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in an `OrderedDict` to name the individual layers and operations. Note that a dictionary keys must be unique, so _each operation must have a different name_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:42.300216Z",
     "start_time": "2021-05-26T22:26:42.289009Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=4, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=4, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1',   nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2',   nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "          ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and biases\n",
    "\n",
    "The weights and such are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with `model.fc1.weight` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:42.972699Z",
     "start_time": "2021-05-26T22:26:42.963913Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[ 0.0080, -0.0015, -0.0095,  ..., -0.0043, -0.0260, -0.0024],\n        [-0.0038,  0.0009,  0.0130,  ...,  0.0121, -0.0051, -0.0155],\n        [-0.0312, -0.0122, -0.0232,  ..., -0.0011,  0.0242, -0.0162],\n        [ 0.0283,  0.0273, -0.0170,  ..., -0.0262,  0.0109, -0.0297]],\n       requires_grad=True)\nParameter containing:\ntensor([ 0.0269,  0.0336, -0.0157,  0.0159], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For custom initialization, we want to modify these tensors in place. These are actually autograd *Variables*, so we need to get back the actual tensors with `model.fc1.weight.data`. Once we have the tensors, we can fill them with zeros (for biases) or random normal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:43.889729Z",
     "start_time": "2021-05-26T22:26:43.883940Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Set biases to all zeros\n",
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:44.084097Z",
     "start_time": "2021-05-26T22:26:44.076738Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.0090, -0.0053, -0.0059,  ..., -0.0023, -0.0045, -0.0065],\n",
       "        [-0.0130,  0.0054, -0.0017,  ...,  0.0110,  0.0073,  0.0136],\n",
       "        [ 0.0057, -0.0123, -0.0221,  ...,  0.0003, -0.0053,  0.0019],\n",
       "        [-0.0118,  0.0092, -0.0084,  ...,  0.0028,  0.0013, -0.0146]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Forward pass\n",
    "\n",
    "Now that we have a network, let's see what happens when we pass in an image. This is called the forward pass. We're going to convert the image data into a tensor, then pass it through the operations defined by the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:44.506324Z",
     "start_time": "2021-05-26T22:26:44.491847Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grab some data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:44.596541Z",
     "start_time": "2021-05-26T22:26:44.594169Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:44.851894Z",
     "start_time": "2021-05-26T22:26:44.845888Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=4, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=4, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:46.121246Z",
     "start_time": "2021-05-26T22:26:46.112022Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "# or images.resize_(images.shape[0], 1, 784) to not automatically get batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:46.519895Z",
     "start_time": "2021-05-26T22:26:46.514137Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "img_idx = 0\n",
    "images[img_idx,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:47.945952Z",
     "start_time": "2021-05-26T22:26:47.888846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Forward pass through the network\n",
    "img_idx = 0\n",
    "ps = model(images[img_idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:50.561845Z",
     "start_time": "2021-05-26T22:26:50.411449Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x648 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 349,
       "height": 195
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "img = images[img_idx]\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our network has basically no idea what this digit is. It's because we haven't trained it yet, all the weights are random!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "The network we built isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
    "\n",
    "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "For single layer networks, gradient descent is simple to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks, although it's straightforward once you learn about it. \n",
    "\n",
    "This is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/w1_backprop_graph.png' width=400px>\n",
    "\n",
    "In the forward pass through the network, our data and operations go from right to left here. To train the weights with gradient descent, we propagate the gradient of the cost backwards through the network. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial w_1} = \\frac{\\partial l_1}{\\partial w_1} \\frac{\\partial s}{\\partial l_1} \\frac{\\partial l_2}{\\partial s} \\frac{\\partial \\ell}{\\partial l_2}\n",
    "$$\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "w^\\prime = w - \\alpha \\frac{\\partial \\ell}{\\partial w}\n",
    "$$\n",
    "\n",
    "The learning rate is set such that the weight update steps are small enough that the iterative method settles in a minimum.\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Torch provides a module, `autograd`, for automatically calculating the gradient of tensors. It does this by keeping track of operations performed on tensors. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`.\n",
    "\n",
    "You can turn off gradients for a block of code with the `torch.no_grad()` content:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:52.867509Z",
     "start_time": "2021-05-26T22:26:52.860629Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.4621, -0.1680],\n        [ 1.5843,  0.7647]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:53.383436Z",
     "start_time": "2021-05-26T22:26:53.375536Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[2.1377, 0.0282],\n        [2.5101, 0.5847]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the operation that created `y`, a power operation `PowBackward0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:53.870654Z",
     "start_time": "2021-05-26T22:26:53.867424Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PowBackward0 object at 0x0000018A96B35188>\n"
     ]
    }
   ],
   "source": [
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autgrad module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor. Let's reduce the tensor `y` to a scalar value, the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:54.831912Z",
     "start_time": "2021-05-26T22:26:54.824631Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.3152, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the gradients for `x` and `y` but they are empty currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:55.546143Z",
     "start_time": "2021-05-26T22:26:55.541213Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradients, you need to run the `.backward` method on a Variable, `z` for example. This will calculate the gradient for `z` with respect to `x`\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:56.607560Z",
     "start_time": "2021-05-26T22:26:56.594993Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.7310, -0.0840],\n        [ 0.7922,  0.3823]])\ntensor([[-0.7310, -0.0840],\n        [ 0.7922,  0.3823]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the cost, then, go backwards to calculate the gradients with respect to the cost. Once we have the gradients we can make a gradient descent step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll build a network with `nn.Sequential` here. Only difference from the last part is I'm not actually using softmax on the output, but instead just using the raw output from the last layer. This is because the output from softmax is a probability distribution. Often, the output will have values really close to zero or really close to one. Due to [inaccuracies with representing numbers as floating points](https://docs.python.org/3/tutorial/floatingpoint.html), computations with a softmax output can lose accuracy and become unstable. To get around this, we'll use the raw output, called the **logits**, to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:56.944759Z",
     "start_time": "2021-05-26T22:26:56.936939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size  = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('logits', nn.Linear(hidden_sizes[1], output_size))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network!\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:57.317614Z",
     "start_time": "2021-05-26T22:26:57.313022Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider just one learning step before looping through all the data. The general process with PyTorch:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "Below I'll go through one training step and print out the weights and gradients so you can see how it changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:27:07.408433Z",
     "start_time": "2021-05-26T22:27:07.373358Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial weights -  Parameter containing:\ntensor([[-0.0140, -0.0180,  0.0107,  ..., -0.0254,  0.0063, -0.0003],\n        [-0.0276,  0.0129,  0.0312,  ..., -0.0062, -0.0248, -0.0061],\n        [-0.0040,  0.0115, -0.0030,  ...,  0.0262,  0.0209, -0.0196],\n        ...,\n        [-0.0085,  0.0102,  0.0106,  ..., -0.0335,  0.0187, -0.0294],\n        [ 0.0234, -0.0106, -0.0057,  ...,  0.0194,  0.0099,  0.0149],\n        [ 0.0060,  0.0306,  0.0026,  ..., -0.0020,  0.0112, -0.0074]],\n       requires_grad=True)\nGradient - tensor([[ 0.0036,  0.0036,  0.0036,  ...,  0.0036,  0.0036,  0.0036],\n        [-0.0006, -0.0006, -0.0006,  ..., -0.0006, -0.0006, -0.0006],\n        [-0.0024, -0.0024, -0.0024,  ..., -0.0024, -0.0024, -0.0024],\n        ...,\n        [ 0.0006,  0.0006,  0.0006,  ...,  0.0006,  0.0006,  0.0006],\n        [-0.0044, -0.0044, -0.0044,  ..., -0.0044, -0.0044, -0.0044],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model.fc1.weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(16, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:27:07.915247Z",
     "start_time": "2021-05-26T22:27:07.908155Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Updated weights -  Parameter containing:\ntensor([[-0.0140, -0.0181,  0.0106,  ..., -0.0255,  0.0063, -0.0003],\n        [-0.0276,  0.0129,  0.0312,  ..., -0.0062, -0.0248, -0.0061],\n        [-0.0039,  0.0116, -0.0030,  ...,  0.0263,  0.0210, -0.0196],\n        ...,\n        [-0.0085,  0.0102,  0.0106,  ..., -0.0335,  0.0186, -0.0294],\n        [ 0.0235, -0.0105, -0.0057,  ...,  0.0194,  0.0100,  0.0149],\n        [ 0.0060,  0.0306,  0.0026,  ..., -0.0020,  0.0112, -0.0074]],\n       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Updated weights - ', model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for real\n",
    "\n",
    "Now we'll put this algorithm into a loop so we can go through all the images. This is fairly straightforward. We'll loop through the mini-batches in our dataset, pass the data through the network to calculate the losses, get the gradients, then run the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:27:08.816179Z",
     "start_time": "2021-05-26T22:27:08.812807Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:27:36.083537Z",
     "start_time": "2021-05-26T22:27:09.280769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n",
      "\tIteration: 0\t Loss: 0.0577\n",
      "\tIteration: 40\t Loss: 2.2975\n",
      "\tIteration: 80\t Loss: 2.2792\n",
      "\tIteration: 120\t Loss: 2.2556\n",
      "\tIteration: 160\t Loss: 2.2323\n",
      "\tIteration: 200\t Loss: 2.2072\n",
      "\tIteration: 240\t Loss: 2.1660\n",
      "\tIteration: 280\t Loss: 2.1453\n",
      "\tIteration: 320\t Loss: 2.1233\n",
      "\tIteration: 360\t Loss: 2.0924\n",
      "\tIteration: 400\t Loss: 2.0521\n",
      "\tIteration: 440\t Loss: 1.9813\n",
      "\tIteration: 480\t Loss: 1.9552\n",
      "\tIteration: 520\t Loss: 1.8864\n",
      "\tIteration: 560\t Loss: 1.8257\n",
      "\tIteration: 600\t Loss: 1.7694\n",
      "\tIteration: 640\t Loss: 1.7441\n",
      "\tIteration: 680\t Loss: 1.6225\n",
      "\tIteration: 720\t Loss: 1.5887\n",
      "\tIteration: 760\t Loss: 1.4847\n",
      "\tIteration: 800\t Loss: 1.4167\n",
      "\tIteration: 840\t Loss: 1.3707\n",
      "\tIteration: 880\t Loss: 1.2962\n",
      "\tIteration: 920\t Loss: 1.2247\n",
      "\tIteration: 960\t Loss: 1.1875\n",
      "\tIteration: 1000\t Loss: 1.1800\n",
      "\tIteration: 1040\t Loss: 1.1078\n",
      "\tIteration: 1080\t Loss: 1.0413\n",
      "\tIteration: 1120\t Loss: 1.0186\n",
      "\tIteration: 1160\t Loss: 0.9772\n",
      "\tIteration: 1200\t Loss: 0.9010\n",
      "\tIteration: 1240\t Loss: 0.8790\n",
      "\tIteration: 1280\t Loss: 0.8666\n",
      "\tIteration: 1320\t Loss: 0.8133\n",
      "\tIteration: 1360\t Loss: 0.8197\n",
      "\tIteration: 1400\t Loss: 0.8298\n",
      "\tIteration: 1440\t Loss: 0.7718\n",
      "\tIteration: 1480\t Loss: 0.7429\n",
      "\tIteration: 1520\t Loss: 0.6878\n",
      "\tIteration: 1560\t Loss: 0.6517\n",
      "\tIteration: 1600\t Loss: 0.6795\n",
      "\tIteration: 1640\t Loss: 0.7119\n",
      "\tIteration: 1680\t Loss: 0.6968\n",
      "\tIteration: 1720\t Loss: 0.6645\n",
      "\tIteration: 1760\t Loss: 0.6214\n",
      "\tIteration: 1800\t Loss: 0.6134\n",
      "\tIteration: 1840\t Loss: 0.5614\n",
      "\tIteration: 1880\t Loss: 0.5841\n",
      "\tIteration: 1920\t Loss: 0.6165\n",
      "\tIteration: 1960\t Loss: 0.6065\n",
      "\tIteration: 2000\t Loss: 0.6265\n",
      "\tIteration: 2040\t Loss: 0.5953\n",
      "\tIteration: 2080\t Loss: 0.5785\n",
      "\tIteration: 2120\t Loss: 0.4998\n",
      "\tIteration: 2160\t Loss: 0.5424\n",
      "\tIteration: 2200\t Loss: 0.5206\n",
      "\tIteration: 2240\t Loss: 0.5621\n",
      "\tIteration: 2280\t Loss: 0.5608\n",
      "\tIteration: 2320\t Loss: 0.5104\n",
      "\tIteration: 2360\t Loss: 0.5240\n",
      "\tIteration: 2400\t Loss: 0.5953\n",
      "\tIteration: 2440\t Loss: 0.4835\n",
      "\tIteration: 2480\t Loss: 0.4797\n",
      "\tIteration: 2520\t Loss: 0.4417\n",
      "\tIteration: 2560\t Loss: 0.5208\n",
      "\tIteration: 2600\t Loss: 0.4585\n",
      "\tIteration: 2640\t Loss: 0.5000\n",
      "\tIteration: 2680\t Loss: 0.5108\n",
      "\tIteration: 2720\t Loss: 0.4579\n",
      "\tIteration: 2760\t Loss: 0.4808\n",
      "\tIteration: 2800\t Loss: 0.5232\n",
      "\tIteration: 2840\t Loss: 0.4443\n",
      "\tIteration: 2880\t Loss: 0.4115\n",
      "\tIteration: 2920\t Loss: 0.4389\n",
      "\tIteration: 2960\t Loss: 0.4822\n",
      "\tIteration: 3000\t Loss: 0.4296\n",
      "\tIteration: 3040\t Loss: 0.4200\n",
      "\tIteration: 3080\t Loss: 0.5014\n",
      "\tIteration: 3120\t Loss: 0.4772\n",
      "\tIteration: 3160\t Loss: 0.4407\n",
      "\tIteration: 3200\t Loss: 0.4339\n",
      "\tIteration: 3240\t Loss: 0.4318\n",
      "\tIteration: 3280\t Loss: 0.4680\n",
      "\tIteration: 3320\t Loss: 0.3823\n",
      "\tIteration: 3360\t Loss: 0.4071\n",
      "\tIteration: 3400\t Loss: 0.4091\n",
      "\tIteration: 3440\t Loss: 0.4071\n",
      "\tIteration: 3480\t Loss: 0.4229\n",
      "\tIteration: 3520\t Loss: 0.4570\n",
      "\tIteration: 3560\t Loss: 0.4221\n",
      "\tIteration: 3600\t Loss: 0.4241\n",
      "\tIteration: 3640\t Loss: 0.4015\n",
      "\tIteration: 3680\t Loss: 0.3777\n",
      "\tIteration: 3720\t Loss: 0.3445\n",
      "Epoch: 2/3\n",
      "\tIteration: 0\t Loss: 0.0075\n",
      "\tIteration: 40\t Loss: 0.3839\n",
      "\tIteration: 80\t Loss: 0.3693\n",
      "\tIteration: 120\t Loss: 0.3422\n",
      "\tIteration: 160\t Loss: 0.3933\n",
      "\tIteration: 200\t Loss: 0.3718\n",
      "\tIteration: 240\t Loss: 0.4313\n",
      "\tIteration: 280\t Loss: 0.3985\n",
      "\tIteration: 320\t Loss: 0.4014\n",
      "\tIteration: 360\t Loss: 0.3387\n",
      "\tIteration: 400\t Loss: 0.3779\n",
      "\tIteration: 440\t Loss: 0.4367\n",
      "\tIteration: 480\t Loss: 0.4414\n",
      "\tIteration: 520\t Loss: 0.3799\n",
      "\tIteration: 560\t Loss: 0.3830\n",
      "\tIteration: 600\t Loss: 0.4134\n",
      "\tIteration: 640\t Loss: 0.4262\n",
      "\tIteration: 680\t Loss: 0.3933\n",
      "\tIteration: 720\t Loss: 0.3837\n",
      "\tIteration: 760\t Loss: 0.3486\n",
      "\tIteration: 800\t Loss: 0.4192\n",
      "\tIteration: 840\t Loss: 0.3179\n",
      "\tIteration: 880\t Loss: 0.3322\n",
      "\tIteration: 920\t Loss: 0.3732\n",
      "\tIteration: 960\t Loss: 0.3253\n",
      "\tIteration: 1000\t Loss: 0.3745\n",
      "\tIteration: 1040\t Loss: 0.3330\n",
      "\tIteration: 1080\t Loss: 0.4126\n",
      "\tIteration: 1120\t Loss: 0.3392\n",
      "\tIteration: 1160\t Loss: 0.3894\n",
      "\tIteration: 1200\t Loss: 0.3863\n",
      "\tIteration: 1240\t Loss: 0.3622\n",
      "\tIteration: 1280\t Loss: 0.3932\n",
      "\tIteration: 1320\t Loss: 0.3606\n",
      "\tIteration: 1360\t Loss: 0.3781\n",
      "\tIteration: 1400\t Loss: 0.3520\n",
      "\tIteration: 1440\t Loss: 0.3491\n",
      "\tIteration: 1480\t Loss: 0.3465\n",
      "\tIteration: 1520\t Loss: 0.3240\n",
      "\tIteration: 1560\t Loss: 0.3341\n",
      "\tIteration: 1600\t Loss: 0.3514\n",
      "\tIteration: 1640\t Loss: 0.3576\n",
      "\tIteration: 1680\t Loss: 0.3625\n",
      "\tIteration: 1720\t Loss: 0.3304\n",
      "\tIteration: 1760\t Loss: 0.3244\n",
      "\tIteration: 1800\t Loss: 0.3605\n",
      "\tIteration: 1840\t Loss: 0.3575\n",
      "\tIteration: 1880\t Loss: 0.3195\n",
      "\tIteration: 1920\t Loss: 0.3330\n",
      "\tIteration: 1960\t Loss: 0.3246\n",
      "\tIteration: 2000\t Loss: 0.3316\n",
      "\tIteration: 2040\t Loss: 0.3196\n",
      "\tIteration: 2080\t Loss: 0.3657\n",
      "\tIteration: 2120\t Loss: 0.3443\n",
      "\tIteration: 2160\t Loss: 0.3374\n",
      "\tIteration: 2200\t Loss: 0.3344\n",
      "\tIteration: 2240\t Loss: 0.3183\n",
      "\tIteration: 2280\t Loss: 0.3599\n",
      "\tIteration: 2320\t Loss: 0.3191\n",
      "\tIteration: 2360\t Loss: 0.3359\n",
      "\tIteration: 2400\t Loss: 0.3479\n",
      "\tIteration: 2440\t Loss: 0.3282\n",
      "\tIteration: 2480\t Loss: 0.3243\n",
      "\tIteration: 2520\t Loss: 0.3843\n",
      "\tIteration: 2560\t Loss: 0.3718\n",
      "\tIteration: 2600\t Loss: 0.3132\n",
      "\tIteration: 2640\t Loss: 0.3206\n",
      "\tIteration: 2680\t Loss: 0.4018\n",
      "\tIteration: 2720\t Loss: 0.2950\n",
      "\tIteration: 2760\t Loss: 0.3585\n",
      "\tIteration: 2800\t Loss: 0.3661\n",
      "\tIteration: 2840\t Loss: 0.3607\n",
      "\tIteration: 2880\t Loss: 0.3527\n",
      "\tIteration: 2920\t Loss: 0.2806\n",
      "\tIteration: 2960\t Loss: 0.3620\n",
      "\tIteration: 3000\t Loss: 0.3399\n",
      "\tIteration: 3040\t Loss: 0.2909\n",
      "\tIteration: 3080\t Loss: 0.3430\n",
      "\tIteration: 3120\t Loss: 0.3187\n",
      "\tIteration: 3160\t Loss: 0.3480\n",
      "\tIteration: 3200\t Loss: 0.3272\n",
      "\tIteration: 3240\t Loss: 0.3179\n",
      "\tIteration: 3280\t Loss: 0.3265\n",
      "\tIteration: 3320\t Loss: 0.3287\n",
      "\tIteration: 3360\t Loss: 0.3302\n",
      "\tIteration: 3400\t Loss: 0.3689\n",
      "\tIteration: 3440\t Loss: 0.3364\n",
      "\tIteration: 3480\t Loss: 0.2695\n",
      "\tIteration: 3520\t Loss: 0.3112\n",
      "\tIteration: 3560\t Loss: 0.2812\n",
      "\tIteration: 3600\t Loss: 0.3814\n",
      "\tIteration: 3640\t Loss: 0.3147\n",
      "\tIteration: 3680\t Loss: 0.2717\n",
      "\tIteration: 3720\t Loss: 0.3368\n",
      "Epoch: 3/3\n",
      "\tIteration: 0\t Loss: 0.0062\n",
      "\tIteration: 40\t Loss: 0.2454\n",
      "\tIteration: 80\t Loss: 0.3007\n",
      "\tIteration: 120\t Loss: 0.2953\n",
      "\tIteration: 160\t Loss: 0.3079\n",
      "\tIteration: 200\t Loss: 0.2665\n",
      "\tIteration: 240\t Loss: 0.3175\n",
      "\tIteration: 280\t Loss: 0.3170\n",
      "\tIteration: 320\t Loss: 0.3900\n",
      "\tIteration: 360\t Loss: 0.3030\n",
      "\tIteration: 400\t Loss: 0.3410\n",
      "\tIteration: 440\t Loss: 0.3019\n",
      "\tIteration: 480\t Loss: 0.3009\n",
      "\tIteration: 520\t Loss: 0.3459\n",
      "\tIteration: 560\t Loss: 0.2698\n",
      "\tIteration: 600\t Loss: 0.2085\n",
      "\tIteration: 640\t Loss: 0.2899\n",
      "\tIteration: 680\t Loss: 0.2655\n",
      "\tIteration: 720\t Loss: 0.3405\n",
      "\tIteration: 760\t Loss: 0.2818\n",
      "\tIteration: 800\t Loss: 0.3963\n",
      "\tIteration: 840\t Loss: 0.3229\n",
      "\tIteration: 880\t Loss: 0.2688\n",
      "\tIteration: 920\t Loss: 0.2875\n",
      "\tIteration: 960\t Loss: 0.3100\n",
      "\tIteration: 1000\t Loss: 0.2834\n",
      "\tIteration: 1040\t Loss: 0.3038\n",
      "\tIteration: 1080\t Loss: 0.3586\n",
      "\tIteration: 1120\t Loss: 0.3143\n",
      "\tIteration: 1160\t Loss: 0.2858\n",
      "\tIteration: 1200\t Loss: 0.3217\n",
      "\tIteration: 1240\t Loss: 0.3292\n",
      "\tIteration: 1280\t Loss: 0.3321\n",
      "\tIteration: 1320\t Loss: 0.2505\n",
      "\tIteration: 1360\t Loss: 0.3077\n",
      "\tIteration: 1400\t Loss: 0.2681\n",
      "\tIteration: 1440\t Loss: 0.2861\n",
      "\tIteration: 1480\t Loss: 0.3948\n",
      "\tIteration: 1520\t Loss: 0.2737\n",
      "\tIteration: 1560\t Loss: 0.2688\n",
      "\tIteration: 1600\t Loss: 0.2613\n",
      "\tIteration: 1640\t Loss: 0.3087\n",
      "\tIteration: 1680\t Loss: 0.3012\n",
      "\tIteration: 1720\t Loss: 0.3224\n",
      "\tIteration: 1760\t Loss: 0.3030\n",
      "\tIteration: 1800\t Loss: 0.3223\n",
      "\tIteration: 1840\t Loss: 0.3129\n",
      "\tIteration: 1880\t Loss: 0.3315\n",
      "\tIteration: 1920\t Loss: 0.3191\n",
      "\tIteration: 1960\t Loss: 0.3059\n",
      "\tIteration: 2000\t Loss: 0.3273\n",
      "\tIteration: 2040\t Loss: 0.2896\n",
      "\tIteration: 2080\t Loss: 0.2689\n",
      "\tIteration: 2120\t Loss: 0.2290\n",
      "\tIteration: 2160\t Loss: 0.2808\n",
      "\tIteration: 2200\t Loss: 0.2987\n",
      "\tIteration: 2240\t Loss: 0.2691\n",
      "\tIteration: 2280\t Loss: 0.3018\n",
      "\tIteration: 2320\t Loss: 0.2760\n",
      "\tIteration: 2360\t Loss: 0.2968\n",
      "\tIteration: 2400\t Loss: 0.3203\n",
      "\tIteration: 2440\t Loss: 0.3030\n",
      "\tIteration: 2480\t Loss: 0.2879\n",
      "\tIteration: 2520\t Loss: 0.2541\n",
      "\tIteration: 2560\t Loss: 0.2728\n",
      "\tIteration: 2600\t Loss: 0.3307\n",
      "\tIteration: 2640\t Loss: 0.3637\n",
      "\tIteration: 2680\t Loss: 0.2403\n",
      "\tIteration: 2720\t Loss: 0.2826\n",
      "\tIteration: 2760\t Loss: 0.2860\n",
      "\tIteration: 2800\t Loss: 0.2693\n",
      "\tIteration: 2840\t Loss: 0.3395\n",
      "\tIteration: 2880\t Loss: 0.3057\n",
      "\tIteration: 2920\t Loss: 0.3225\n",
      "\tIteration: 2960\t Loss: 0.2874\n",
      "\tIteration: 3000\t Loss: 0.2389\n",
      "\tIteration: 3040\t Loss: 0.2786\n",
      "\tIteration: 3080\t Loss: 0.2697\n",
      "\tIteration: 3120\t Loss: 0.2875\n",
      "\tIteration: 3160\t Loss: 0.3146\n",
      "\tIteration: 3200\t Loss: 0.2813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration: 3240\t Loss: 0.2681\n",
      "\tIteration: 3280\t Loss: 0.2929\n",
      "\tIteration: 3320\t Loss: 0.2956\n",
      "\tIteration: 3360\t Loss: 0.3227\n",
      "\tIteration: 3400\t Loss: 0.2747\n",
      "\tIteration: 3440\t Loss: 0.2837\n",
      "\tIteration: 3480\t Loss: 0.3219\n",
      "\tIteration: 3520\t Loss: 0.2908\n",
      "\tIteration: 3560\t Loss: 0.3121\n",
      "\tIteration: 3600\t Loss: 0.2743\n",
      "\tIteration: 3640\t Loss: 0.2899\n",
      "\tIteration: 3680\t Loss: 0.2687\n",
      "\tIteration: 3720\t Loss: 0.2779\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "print_every = 40\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the network trained, we can check out it's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:30:00.206666Z",
     "start_time": "2021-05-26T22:29:59.954325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 195,
       "width": 349
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = F.softmax(logits, dim=1)\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our network is brilliant. It can accurately predict the digits in our images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\">EMNIST Classification: Exercise</h2>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 1:</h3>\n",
    "  <p>Now it's your turn to build a simple network, use any method I've covered so far. In the next notebook, you'll learn how to train a network so it can make good predictions.</p>\n",
    "  <p>Build a network to classify the MNIST images with 3 hidden layers. Use 16 units in the first hidden layer, 32 units in the second layer, and 8 units in the third layer. Each hidden layer should have a ReLU activation function, and use softmax on the output layer.</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Your network here\n",
    "class Strive_Network(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784,16)\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(32,8 )\n",
    "        self.fc4 = nn.Linear(8, 10)\n",
    "        \n",
    "    # Forward pass through the network, returns the output logits\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Strive_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x648 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 349,
       "height": 195
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Run this cell with your model to make sure it works\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 2:</h3>\n",
    "  <p>Train your network implementing the Pytorch training loop and <strong style=\"color:#01ff84\">after each epoch, use the model for predicting the test (validation) MNIST data.</strong></p>\n",
    "  <p>Note: If your model does not fit with the final softmax layer, you can remove this layer.</p>\n",
    "  <p>Hint: <a href=\"https://discuss.pytorch.org/t/training-loop-checking-validation-accuracy/78399\">Training loop checking validation accuracy\n",
    "</a></p>\n",
    "  <p>Research about <code>model.train()</code>, <code>model.eval()</code> and <code>with torch.no_grad()</code> in Pytorch.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/3\n",
      "\tIteration: 0\t Loss: 0.0576\n",
      "\tIteration: 40\t Loss: 2.3025\n",
      "\tIteration: 80\t Loss: 2.3007\n",
      "\tIteration: 120\t Loss: 2.2958\n",
      "\tIteration: 160\t Loss: 2.2951\n",
      "\tIteration: 200\t Loss: 2.2764\n",
      "\tIteration: 240\t Loss: 2.2622\n",
      "\tIteration: 280\t Loss: 2.2162\n",
      "\tIteration: 320\t Loss: 2.1882\n",
      "\tIteration: 360\t Loss: 2.1519\n",
      "\tIteration: 400\t Loss: 2.1619\n",
      "\tIteration: 440\t Loss: 2.1072\n",
      "\tIteration: 480\t Loss: 2.0318\n",
      "\tIteration: 520\t Loss: 2.0262\n",
      "\tIteration: 560\t Loss: 1.9758\n",
      "\tIteration: 600\t Loss: 1.9437\n",
      "\tIteration: 640\t Loss: 1.9728\n",
      "\tIteration: 680\t Loss: 1.9158\n",
      "\tIteration: 720\t Loss: 1.8813\n",
      "\tIteration: 760\t Loss: 1.8794\n",
      "\tIteration: 800\t Loss: 1.8474\n",
      "\tIteration: 840\t Loss: 1.8265\n",
      "\tIteration: 880\t Loss: 1.8121\n",
      "\tIteration: 920\t Loss: 1.8167\n",
      "\tIteration: 960\t Loss: 1.7942\n",
      "\tIteration: 1000\t Loss: 1.7562\n",
      "\tIteration: 1040\t Loss: 1.7454\n",
      "\tIteration: 1080\t Loss: 1.7625\n",
      "\tIteration: 1120\t Loss: 1.7504\n",
      "\tIteration: 1160\t Loss: 1.7582\n",
      "\tIteration: 1200\t Loss: 1.7426\n",
      "\tIteration: 1240\t Loss: 1.7412\n",
      "\tIteration: 1280\t Loss: 1.7044\n",
      "\tIteration: 1320\t Loss: 1.7060\n",
      "\tIteration: 1360\t Loss: 1.7011\n",
      "\tIteration: 1400\t Loss: 1.7008\n",
      "\tIteration: 1440\t Loss: 1.6870\n",
      "\tIteration: 1480\t Loss: 1.6712\n",
      "\tIteration: 1520\t Loss: 1.6908\n",
      "\tIteration: 1560\t Loss: 1.6718\n",
      "\tIteration: 1600\t Loss: 1.7181\n",
      "\tIteration: 1640\t Loss: 1.6619\n",
      "\tIteration: 1680\t Loss: 1.6927\n",
      "\tIteration: 1720\t Loss: 1.6692\n",
      "\tIteration: 1760\t Loss: 1.6685\n",
      "\tIteration: 1800\t Loss: 1.6566\n",
      "\tIteration: 1840\t Loss: 1.6592\n",
      "\tIteration: 1880\t Loss: 1.6760\n",
      "\tIteration: 1920\t Loss: 1.6738\n",
      "\tIteration: 1960\t Loss: 1.6804\n",
      "\tIteration: 2000\t Loss: 1.6178\n",
      "\tIteration: 2040\t Loss: 1.6580\n",
      "\tIteration: 2080\t Loss: 1.6530\n",
      "\tIteration: 2120\t Loss: 1.6532\n",
      "\tIteration: 2160\t Loss: 1.7066\n",
      "\tIteration: 2200\t Loss: 1.6619\n",
      "\tIteration: 2240\t Loss: 1.6949\n",
      "\tIteration: 2280\t Loss: 1.6864\n",
      "\tIteration: 2320\t Loss: 1.6897\n",
      "\tIteration: 2360\t Loss: 1.6432\n",
      "\tIteration: 2400\t Loss: 1.6426\n",
      "\tIteration: 2440\t Loss: 1.6060\n",
      "\tIteration: 2480\t Loss: 1.6249\n",
      "\tIteration: 2520\t Loss: 1.6332\n",
      "\tIteration: 2560\t Loss: 1.6428\n",
      "\tIteration: 2600\t Loss: 1.6483\n",
      "\tIteration: 2640\t Loss: 1.6408\n",
      "\tIteration: 2680\t Loss: 1.6325\n",
      "\tIteration: 2720\t Loss: 1.6030\n",
      "\tIteration: 2760\t Loss: 1.6265\n",
      "\tIteration: 2800\t Loss: 1.6487\n",
      "\tIteration: 2840\t Loss: 1.6244\n",
      "\tIteration: 2880\t Loss: 1.6171\n",
      "\tIteration: 2920\t Loss: 1.6085\n",
      "\tIteration: 2960\t Loss: 1.6199\n",
      "\tIteration: 3000\t Loss: 1.6354\n",
      "\tIteration: 3040\t Loss: 1.6507\n",
      "\tIteration: 3080\t Loss: 1.6574\n",
      "\tIteration: 3120\t Loss: 1.6388\n",
      "\tIteration: 3160\t Loss: 1.5907\n",
      "\tIteration: 3200\t Loss: 1.5989\n",
      "\tIteration: 3240\t Loss: 1.6535\n",
      "\tIteration: 3280\t Loss: 1.6203\n",
      "\tIteration: 3320\t Loss: 1.6351\n",
      "\tIteration: 3360\t Loss: 1.6425\n",
      "\tIteration: 3400\t Loss: 1.6628\n",
      "\tIteration: 3440\t Loss: 1.6440\n",
      "\tIteration: 3480\t Loss: 1.6317\n",
      "\tIteration: 3520\t Loss: 1.6191\n",
      "\tIteration: 3560\t Loss: 1.6182\n",
      "\tIteration: 3600\t Loss: 1.6504\n",
      "\tIteration: 3640\t Loss: 1.6374\n",
      "\tIteration: 3680\t Loss: 1.6366\n",
      "\tIteration: 3720\t Loss: 1.6285\n",
      "Accuracy of the network: 68 %\n",
      "Epoch: 2/3\n",
      "\tIteration: 0\t Loss: 0.0491\n",
      "\tIteration: 40\t Loss: 1.6226\n",
      "\tIteration: 80\t Loss: 1.5950\n",
      "\tIteration: 120\t Loss: 1.6119\n",
      "\tIteration: 160\t Loss: 1.6352\n",
      "\tIteration: 200\t Loss: 1.6343\n",
      "\tIteration: 240\t Loss: 1.5954\n",
      "\tIteration: 280\t Loss: 1.6228\n",
      "\tIteration: 320\t Loss: 1.6062\n",
      "\tIteration: 360\t Loss: 1.5809\n",
      "\tIteration: 400\t Loss: 1.6355\n",
      "\tIteration: 440\t Loss: 1.6009\n",
      "\tIteration: 480\t Loss: 1.6086\n",
      "\tIteration: 520\t Loss: 1.6078\n",
      "\tIteration: 560\t Loss: 1.6111\n",
      "\tIteration: 600\t Loss: 1.6056\n",
      "\tIteration: 640\t Loss: 1.5792\n",
      "\tIteration: 680\t Loss: 1.6340\n",
      "\tIteration: 720\t Loss: 1.6129\n",
      "\tIteration: 760\t Loss: 1.6159\n",
      "\tIteration: 800\t Loss: 1.6495\n",
      "\tIteration: 840\t Loss: 1.5908\n",
      "\tIteration: 880\t Loss: 1.6222\n",
      "\tIteration: 920\t Loss: 1.6042\n",
      "\tIteration: 960\t Loss: 1.5957\n",
      "\tIteration: 1000\t Loss: 1.6148\n",
      "\tIteration: 1040\t Loss: 1.6110\n",
      "\tIteration: 1080\t Loss: 1.6185\n",
      "\tIteration: 1120\t Loss: 1.6150\n",
      "\tIteration: 1160\t Loss: 1.6281\n",
      "\tIteration: 1200\t Loss: 1.5893\n",
      "\tIteration: 1240\t Loss: 1.5962\n",
      "\tIteration: 1280\t Loss: 1.6030\n",
      "\tIteration: 1320\t Loss: 1.5982\n",
      "\tIteration: 1360\t Loss: 1.6340\n",
      "\tIteration: 1400\t Loss: 1.6084\n",
      "\tIteration: 1440\t Loss: 1.6043\n",
      "\tIteration: 1480\t Loss: 1.6283\n",
      "\tIteration: 1520\t Loss: 1.5821\n",
      "\tIteration: 1560\t Loss: 1.6126\n",
      "\tIteration: 1600\t Loss: 1.5989\n",
      "\tIteration: 1640\t Loss: 1.6116\n",
      "\tIteration: 1680\t Loss: 1.6035\n",
      "\tIteration: 1720\t Loss: 1.6428\n",
      "\tIteration: 1760\t Loss: 1.6088\n",
      "\tIteration: 1800\t Loss: 1.6342\n",
      "\tIteration: 1840\t Loss: 1.6059\n",
      "\tIteration: 1880\t Loss: 1.5824\n",
      "\tIteration: 1920\t Loss: 1.5890\n",
      "\tIteration: 1960\t Loss: 1.5965\n",
      "\tIteration: 2000\t Loss: 1.5919\n",
      "\tIteration: 2040\t Loss: 1.5997\n",
      "\tIteration: 2080\t Loss: 1.5925\n",
      "\tIteration: 2120\t Loss: 1.6186\n",
      "\tIteration: 2160\t Loss: 1.6069\n",
      "\tIteration: 2200\t Loss: 1.6053\n",
      "\tIteration: 2240\t Loss: 1.5865\n",
      "\tIteration: 2280\t Loss: 1.6036\n",
      "\tIteration: 2320\t Loss: 1.5964\n",
      "\tIteration: 2360\t Loss: 1.5995\n",
      "\tIteration: 2400\t Loss: 1.5781\n",
      "\tIteration: 2440\t Loss: 1.6292\n",
      "\tIteration: 2480\t Loss: 1.6046\n",
      "\tIteration: 2520\t Loss: 1.6044\n",
      "\tIteration: 2560\t Loss: 1.6347\n",
      "\tIteration: 2600\t Loss: 1.6234\n",
      "\tIteration: 2640\t Loss: 1.6182\n",
      "\tIteration: 2680\t Loss: 1.6195\n",
      "\tIteration: 2720\t Loss: 1.5995\n",
      "\tIteration: 2760\t Loss: 1.5927\n",
      "\tIteration: 2800\t Loss: 1.5742\n",
      "\tIteration: 2840\t Loss: 1.5884\n",
      "\tIteration: 2880\t Loss: 1.5815\n",
      "\tIteration: 2920\t Loss: 1.6275\n",
      "\tIteration: 2960\t Loss: 1.6073\n",
      "\tIteration: 3000\t Loss: 1.5792\n",
      "\tIteration: 3040\t Loss: 1.6235\n",
      "\tIteration: 3080\t Loss: 1.5979\n",
      "\tIteration: 3120\t Loss: 1.6233\n",
      "\tIteration: 3160\t Loss: 1.5865\n",
      "\tIteration: 3200\t Loss: 1.6291\n",
      "\tIteration: 3240\t Loss: 1.5823\n",
      "\tIteration: 3280\t Loss: 1.5832\n",
      "\tIteration: 3320\t Loss: 1.5894\n",
      "\tIteration: 3360\t Loss: 1.6194\n",
      "\tIteration: 3400\t Loss: 1.5970\n",
      "\tIteration: 3440\t Loss: 1.6046\n",
      "\tIteration: 3480\t Loss: 1.5917\n",
      "\tIteration: 3520\t Loss: 1.5629\n",
      "\tIteration: 3560\t Loss: 1.5672\n",
      "\tIteration: 3600\t Loss: 1.5686\n",
      "\tIteration: 3640\t Loss: 1.6178\n",
      "\tIteration: 3680\t Loss: 1.5833\n",
      "\tIteration: 3720\t Loss: 1.5742\n",
      "Accuracy of the network: 88 %\n",
      "Epoch: 3/3\n",
      "\tIteration: 0\t Loss: 0.0378\n",
      "\tIteration: 40\t Loss: 1.6040\n",
      "\tIteration: 80\t Loss: 1.6005\n",
      "\tIteration: 120\t Loss: 1.6030\n",
      "\tIteration: 160\t Loss: 1.5591\n",
      "\tIteration: 200\t Loss: 1.5946\n",
      "\tIteration: 240\t Loss: 1.6114\n",
      "\tIteration: 280\t Loss: 1.6094\n",
      "\tIteration: 320\t Loss: 1.5810\n",
      "\tIteration: 360\t Loss: 1.6187\n",
      "\tIteration: 400\t Loss: 1.6131\n",
      "\tIteration: 440\t Loss: 1.6061\n",
      "\tIteration: 480\t Loss: 1.6022\n",
      "\tIteration: 520\t Loss: 1.6171\n",
      "\tIteration: 560\t Loss: 1.6073\n",
      "\tIteration: 600\t Loss: 1.5946\n",
      "\tIteration: 640\t Loss: 1.5834\n",
      "\tIteration: 680\t Loss: 1.5980\n",
      "\tIteration: 720\t Loss: 1.5822\n",
      "\tIteration: 760\t Loss: 1.5967\n",
      "\tIteration: 800\t Loss: 1.5776\n",
      "\tIteration: 840\t Loss: 1.5803\n",
      "\tIteration: 880\t Loss: 1.6337\n",
      "\tIteration: 920\t Loss: 1.5831\n",
      "\tIteration: 960\t Loss: 1.5886\n",
      "\tIteration: 1000\t Loss: 1.6040\n",
      "\tIteration: 1040\t Loss: 1.6146\n",
      "\tIteration: 1080\t Loss: 1.6214\n",
      "\tIteration: 1120\t Loss: 1.5909\n",
      "\tIteration: 1160\t Loss: 1.6007\n",
      "\tIteration: 1200\t Loss: 1.6068\n",
      "\tIteration: 1240\t Loss: 1.5925\n",
      "\tIteration: 1280\t Loss: 1.5687\n",
      "\tIteration: 1320\t Loss: 1.6044\n",
      "\tIteration: 1360\t Loss: 1.5949\n",
      "\tIteration: 1400\t Loss: 1.6091\n",
      "\tIteration: 1440\t Loss: 1.6154\n",
      "\tIteration: 1480\t Loss: 1.5840\n",
      "\tIteration: 1520\t Loss: 1.6235\n",
      "\tIteration: 1560\t Loss: 1.5777\n",
      "\tIteration: 1600\t Loss: 1.5569\n",
      "\tIteration: 1640\t Loss: 1.5775\n",
      "\tIteration: 1680\t Loss: 1.6164\n",
      "\tIteration: 1720\t Loss: 1.5612\n",
      "\tIteration: 1760\t Loss: 1.5857\n",
      "\tIteration: 1800\t Loss: 1.6100\n",
      "\tIteration: 1840\t Loss: 1.5968\n",
      "\tIteration: 1880\t Loss: 1.6115\n",
      "\tIteration: 1920\t Loss: 1.6223\n",
      "\tIteration: 1960\t Loss: 1.5708\n",
      "\tIteration: 2000\t Loss: 1.6316\n",
      "\tIteration: 2040\t Loss: 1.6337\n",
      "\tIteration: 2080\t Loss: 1.5960\n",
      "\tIteration: 2120\t Loss: 1.6205\n",
      "\tIteration: 2160\t Loss: 1.6210\n",
      "\tIteration: 2200\t Loss: 1.6150\n",
      "\tIteration: 2240\t Loss: 1.6446\n",
      "\tIteration: 2280\t Loss: 1.6065\n",
      "\tIteration: 2320\t Loss: 1.5847\n",
      "\tIteration: 2360\t Loss: 1.6021\n",
      "\tIteration: 2400\t Loss: 1.6335\n",
      "\tIteration: 2440\t Loss: 1.6118\n",
      "\tIteration: 2480\t Loss: 1.6031\n",
      "\tIteration: 2520\t Loss: 1.6100\n",
      "\tIteration: 2560\t Loss: 1.6483\n",
      "\tIteration: 2600\t Loss: 1.6350\n",
      "\tIteration: 2640\t Loss: 1.6052\n",
      "\tIteration: 2680\t Loss: 1.6019\n",
      "\tIteration: 2720\t Loss: 1.6219\n",
      "\tIteration: 2760\t Loss: 1.5911\n",
      "\tIteration: 2800\t Loss: 1.6137\n",
      "\tIteration: 2840\t Loss: 1.6009\n",
      "\tIteration: 2880\t Loss: 1.5734\n",
      "\tIteration: 2920\t Loss: 1.5869\n",
      "\tIteration: 2960\t Loss: 1.5688\n",
      "\tIteration: 3000\t Loss: 1.5963\n",
      "\tIteration: 3040\t Loss: 1.6002\n",
      "\tIteration: 3080\t Loss: 1.5738\n",
      "\tIteration: 3120\t Loss: 1.5918\n",
      "\tIteration: 3160\t Loss: 1.6236\n",
      "\tIteration: 3200\t Loss: 1.5881\n",
      "\tIteration: 3240\t Loss: 1.5892\n",
      "\tIteration: 3280\t Loss: 1.5786\n",
      "\tIteration: 3320\t Loss: 1.6231\n",
      "\tIteration: 3360\t Loss: 1.5849\n",
      "\tIteration: 3400\t Loss: 1.5864\n",
      "\tIteration: 3440\t Loss: 1.5940\n",
      "\tIteration: 3480\t Loss: 1.5751\n",
      "\tIteration: 3520\t Loss: 1.5927\n",
      "\tIteration: 3560\t Loss: 1.6322\n",
      "\tIteration: 3600\t Loss: 1.6515\n",
      "\tIteration: 3640\t Loss: 1.5741\n",
      "\tIteration: 3680\t Loss: 1.5503\n",
      "\tIteration: 3720\t Loss: 1.6021\n",
      "Accuracy of the network: 88 %\n"
     ]
    }
   ],
   "source": [
    "## TODO: Your training loop here\n",
    "epochs = 3\n",
    "print_every = 40\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(iter(testloader)):\n",
    "            images.resize_(images.size()[0], 784)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell with your model to make sure it works and predicts well for the validation data\n",
    "images, labels = next(iter(testloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 3:</h3>\n",
    "  <p>Write the code for adding <strong style=\"color:#01ff84\">Early Stopping with patience = 2</strong> to the training loop from scratch.</p>\n",
    "  <p><strong style=\"color:#01ff84\">Hint:</strong> Monitor the Validation loss every epoch, and if in 2 epochs, the validation loss does not improve, stop the training loop with <code>break</code>.</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/5\n",
      "\tIteration: 0\t Loss: 0.0412\n",
      "\tIteration: 40\t Loss: 1.5941\n",
      "\tIteration: 80\t Loss: 1.5747\n",
      "\tIteration: 120\t Loss: 1.5624\n",
      "\tIteration: 160\t Loss: 1.5691\n",
      "\tIteration: 200\t Loss: 1.5825\n",
      "\tIteration: 240\t Loss: 1.6226\n",
      "\tIteration: 280\t Loss: 1.6011\n",
      "\tIteration: 320\t Loss: 1.5766\n",
      "\tIteration: 360\t Loss: 1.5690\n",
      "\tIteration: 400\t Loss: 1.6596\n",
      "\tIteration: 440\t Loss: 1.6236\n",
      "\tIteration: 480\t Loss: 1.5961\n",
      "\tIteration: 520\t Loss: 1.5918\n",
      "\tIteration: 560\t Loss: 1.6314\n",
      "\tIteration: 600\t Loss: 1.5954\n",
      "\tIteration: 640\t Loss: 1.5536\n",
      "\tIteration: 680\t Loss: 1.5903\n",
      "\tIteration: 720\t Loss: 1.6133\n",
      "\tIteration: 760\t Loss: 1.5953\n",
      "\tIteration: 800\t Loss: 1.5702\n",
      "\tIteration: 840\t Loss: 1.5743\n",
      "\tIteration: 880\t Loss: 1.5953\n",
      "\tIteration: 920\t Loss: 1.5875\n",
      "\tIteration: 960\t Loss: 1.6221\n",
      "\tIteration: 1000\t Loss: 1.5977\n",
      "\tIteration: 1040\t Loss: 1.6057\n",
      "\tIteration: 1080\t Loss: 1.5884\n",
      "\tIteration: 1120\t Loss: 1.5746\n",
      "\tIteration: 1160\t Loss: 1.5808\n",
      "\tIteration: 1200\t Loss: 1.5804\n",
      "\tIteration: 1240\t Loss: 1.6025\n",
      "\tIteration: 1280\t Loss: 1.6148\n",
      "\tIteration: 1320\t Loss: 1.5949\n",
      "\tIteration: 1360\t Loss: 1.5934\n",
      "\tIteration: 1400\t Loss: 1.5736\n",
      "\tIteration: 1440\t Loss: 1.6079\n",
      "\tIteration: 1480\t Loss: 1.5694\n",
      "\tIteration: 1520\t Loss: 1.5805\n",
      "\tIteration: 1560\t Loss: 1.6057\n",
      "\tIteration: 1600\t Loss: 1.5987\n",
      "\tIteration: 1640\t Loss: 1.5958\n",
      "\tIteration: 1680\t Loss: 1.5845\n",
      "\tIteration: 1720\t Loss: 1.5566\n",
      "\tIteration: 1760\t Loss: 1.5732\n",
      "\tIteration: 1800\t Loss: 1.6078\n",
      "\tIteration: 1840\t Loss: 1.6257\n",
      "\tIteration: 1880\t Loss: 1.6331\n",
      "\tIteration: 1920\t Loss: 1.5847\n",
      "\tIteration: 1960\t Loss: 1.5661\n",
      "\tIteration: 2000\t Loss: 1.6072\n",
      "\tIteration: 2040\t Loss: 1.5932\n",
      "\tIteration: 2080\t Loss: 1.6022\n",
      "\tIteration: 2120\t Loss: 1.5462\n",
      "\tIteration: 2160\t Loss: 1.5643\n",
      "\tIteration: 2200\t Loss: 1.5660\n",
      "\tIteration: 2240\t Loss: 1.5816\n",
      "\tIteration: 2280\t Loss: 1.5988\n",
      "\tIteration: 2320\t Loss: 1.5953\n",
      "\tIteration: 2360\t Loss: 1.6101\n",
      "\tIteration: 2400\t Loss: 1.5911\n",
      "\tIteration: 2440\t Loss: 1.6391\n",
      "\tIteration: 2480\t Loss: 1.6538\n",
      "\tIteration: 2520\t Loss: 1.5871\n",
      "\tIteration: 2560\t Loss: 1.5941\n",
      "\tIteration: 2600\t Loss: 1.5724\n",
      "\tIteration: 2640\t Loss: 1.6069\n",
      "\tIteration: 2680\t Loss: 1.5962\n",
      "\tIteration: 2720\t Loss: 1.5973\n",
      "\tIteration: 2760\t Loss: 1.5957\n",
      "\tIteration: 2800\t Loss: 1.6053\n",
      "\tIteration: 2840\t Loss: 1.6674\n",
      "\tIteration: 2880\t Loss: 1.6043\n",
      "\tIteration: 2920\t Loss: 1.5756\n",
      "\tIteration: 2960\t Loss: 1.6153\n",
      "\tIteration: 3000\t Loss: 1.5920\n",
      "\tIteration: 3040\t Loss: 1.6890\n",
      "\tIteration: 3080\t Loss: 1.5582\n",
      "\tIteration: 3120\t Loss: 1.6133\n",
      "\tIteration: 3160\t Loss: 1.5699\n",
      "\tIteration: 3200\t Loss: 1.6134\n",
      "\tIteration: 3240\t Loss: 1.5597\n",
      "\tIteration: 3280\t Loss: 1.6354\n",
      "\tIteration: 3320\t Loss: 1.6024\n",
      "\tIteration: 3360\t Loss: 1.6186\n",
      "\tIteration: 3400\t Loss: 1.6085\n",
      "\tIteration: 3440\t Loss: 1.6180\n",
      "\tIteration: 3480\t Loss: 1.5941\n",
      "\tIteration: 3520\t Loss: 1.5928\n",
      "\tIteration: 3560\t Loss: 1.5716\n",
      "\tIteration: 3600\t Loss: 1.6150\n",
      "\tIteration: 3640\t Loss: 1.5921\n",
      "\tIteration: 3680\t Loss: 1.6220\n",
      "\tIteration: 3720\t Loss: 1.5694\n",
      "Accuracy of the network: 87 %\n",
      "Validation loss: 988.8927297592163\n",
      "Epoch: 2/5\n",
      "\tIteration: 0\t Loss: 0.0367\n",
      "\tIteration: 40\t Loss: 1.5748\n",
      "\tIteration: 80\t Loss: 1.5797\n",
      "\tIteration: 120\t Loss: 1.5886\n",
      "\tIteration: 160\t Loss: 1.6031\n",
      "\tIteration: 200\t Loss: 1.5974\n",
      "\tIteration: 240\t Loss: 1.5697\n",
      "\tIteration: 280\t Loss: 1.6190\n",
      "\tIteration: 320\t Loss: 1.6156\n",
      "\tIteration: 360\t Loss: 1.5871\n",
      "\tIteration: 400\t Loss: 1.6306\n",
      "\tIteration: 440\t Loss: 1.5816\n",
      "\tIteration: 480\t Loss: 1.5636\n",
      "\tIteration: 520\t Loss: 1.6367\n",
      "\tIteration: 560\t Loss: 1.6249\n",
      "\tIteration: 600\t Loss: 1.6109\n",
      "\tIteration: 640\t Loss: 1.6096\n",
      "\tIteration: 680\t Loss: 1.6363\n",
      "\tIteration: 720\t Loss: 1.6145\n",
      "\tIteration: 760\t Loss: 1.6427\n",
      "\tIteration: 800\t Loss: 1.6672\n",
      "\tIteration: 840\t Loss: 1.7075\n",
      "\tIteration: 880\t Loss: 1.6610\n",
      "\tIteration: 920\t Loss: 1.6476\n",
      "\tIteration: 960\t Loss: 1.5980\n",
      "\tIteration: 1000\t Loss: 1.6039\n",
      "\tIteration: 1040\t Loss: 1.5911\n",
      "\tIteration: 1080\t Loss: 1.5898\n",
      "\tIteration: 1120\t Loss: 1.6071\n",
      "\tIteration: 1160\t Loss: 1.5775\n",
      "\tIteration: 1200\t Loss: 1.6490\n",
      "\tIteration: 1240\t Loss: 1.6130\n",
      "\tIteration: 1280\t Loss: 1.6401\n",
      "\tIteration: 1320\t Loss: 1.6931\n",
      "\tIteration: 1360\t Loss: 1.5703\n",
      "\tIteration: 1400\t Loss: 1.5933\n",
      "\tIteration: 1440\t Loss: 1.6221\n",
      "\tIteration: 1480\t Loss: 1.5857\n",
      "\tIteration: 1520\t Loss: 1.6078\n",
      "\tIteration: 1560\t Loss: 1.6608\n",
      "\tIteration: 1600\t Loss: 1.6470\n",
      "\tIteration: 1640\t Loss: 1.5783\n",
      "\tIteration: 1680\t Loss: 1.5980\n",
      "\tIteration: 1720\t Loss: 1.6372\n",
      "\tIteration: 1760\t Loss: 1.6526\n",
      "\tIteration: 1800\t Loss: 1.6311\n",
      "\tIteration: 1840\t Loss: 1.6103\n",
      "\tIteration: 1880\t Loss: 1.6003\n",
      "\tIteration: 1920\t Loss: 1.6278\n",
      "\tIteration: 1960\t Loss: 1.6270\n",
      "\tIteration: 2000\t Loss: 1.6225\n",
      "\tIteration: 2040\t Loss: 1.6720\n",
      "\tIteration: 2080\t Loss: 1.5733\n",
      "\tIteration: 2120\t Loss: 1.5972\n",
      "\tIteration: 2160\t Loss: 1.5818\n",
      "\tIteration: 2200\t Loss: 1.5624\n",
      "\tIteration: 2240\t Loss: 1.6156\n",
      "\tIteration: 2280\t Loss: 1.5931\n",
      "\tIteration: 2320\t Loss: 1.6422\n",
      "\tIteration: 2360\t Loss: 1.6104\n",
      "\tIteration: 2400\t Loss: 1.6075\n",
      "\tIteration: 2440\t Loss: 1.6050\n",
      "\tIteration: 2480\t Loss: 1.5911\n",
      "\tIteration: 2520\t Loss: 1.5754\n",
      "\tIteration: 2560\t Loss: 1.5760\n",
      "\tIteration: 2600\t Loss: 1.6751\n",
      "\tIteration: 2640\t Loss: 1.7044\n",
      "\tIteration: 2680\t Loss: 1.6434\n",
      "\tIteration: 2720\t Loss: 1.6192\n",
      "\tIteration: 2760\t Loss: 1.6549\n",
      "\tIteration: 2800\t Loss: 1.6363\n",
      "\tIteration: 2840\t Loss: 1.6240\n",
      "\tIteration: 2880\t Loss: 1.6134\n",
      "\tIteration: 2920\t Loss: 1.6410\n",
      "\tIteration: 2960\t Loss: 1.7225\n",
      "\tIteration: 3000\t Loss: 1.6204\n",
      "\tIteration: 3040\t Loss: 1.5895\n",
      "\tIteration: 3080\t Loss: 1.5845\n",
      "\tIteration: 3120\t Loss: 1.5711\n",
      "\tIteration: 3160\t Loss: 1.5803\n",
      "\tIteration: 3200\t Loss: 1.6121\n",
      "\tIteration: 3240\t Loss: 1.6240\n",
      "\tIteration: 3280\t Loss: 1.5782\n",
      "\tIteration: 3320\t Loss: 1.6207\n",
      "\tIteration: 3360\t Loss: 1.6380\n",
      "\tIteration: 3400\t Loss: 1.6028\n",
      "\tIteration: 3440\t Loss: 1.6636\n",
      "\tIteration: 3480\t Loss: 1.5869\n",
      "\tIteration: 3520\t Loss: 1.6370\n",
      "\tIteration: 3560\t Loss: 1.7316\n",
      "\tIteration: 3600\t Loss: 1.6384\n",
      "\tIteration: 3640\t Loss: 1.5994\n",
      "\tIteration: 3680\t Loss: 1.6071\n",
      "\tIteration: 3720\t Loss: 1.6077\n",
      "Accuracy of the network: 86 %\n",
      "Validation loss: 998.0568175315857\n"
     ]
    }
   ],
   "source": [
    "## TODO: Your training loop here\n",
    "best_validation_loss = np.inf\n",
    "epochs = 5\n",
    "print_every = 40\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    running_validation_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(iter(testloader)):\n",
    "            images.resize_(images.size()[0], 784)\n",
    "            output = model(images)\n",
    "            running_validation_loss += criterion(output, labels).item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network: %d %%' % (\n",
    "        100 * correct / total))\n",
    "    print(f'Validation loss: {running_validation_loss}')\n",
    "    if best_validation_loss > running_validation_loss:\n",
    "        best_validation_loss = running_validation_loss\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Optional:</h3>\n",
    "  <p>Don't you want to use MNIST? Try EMNIST instead! Maybe using the first 10 letters of the alphabet!</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:35:26.981584Z",
     "start_time": "2021-05-26T22:35:26.954522Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will need a custom visualization function\n",
    "def view_classify_emnist(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(list(\"abcdefghij\"), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:50:57.571260Z",
     "start_time": "2021-05-26T22:50:57.322172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data (Preprocessing)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5)) ])\n",
    "def my_collate(batch):\n",
    "    modified_batch = []\n",
    "    for item in batch:\n",
    "        image, label = item\n",
    "        if label < 10: # only the first ten letters\n",
    "            modified_batch.append(item)\n",
    "    return torch.utils.data._utils.collate.default_collate(modified_batch)\n",
    "\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.EMNIST('EMNIST_data/', split=\"letters\", download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, collate_fn=my_collate)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.EMNIST('EMNIST_data/', split=\"letters\", download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=True, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:51:02.493175Z",
     "start_time": "2021-05-26T22:51:02.464301Z"
    }
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:51:03.118421Z",
     "start_time": "2021-05-26T22:51:02.978678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[5].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:51:06.653639Z",
     "start_time": "2021-05-26T22:51:06.647991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 9, 6, 7, 5, 1, 9, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd050a93f2fecfd00da27f8930a2c1c74e92db967b2162384b3e8848f4306dc0d4b",
   "display_name": "Python 3.7.10 64-bit ('deeplearner': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "metadata": {
   "interpreter": {
    "hash": "50a93f2fecfd00da27f8930a2c1c74e92db967b2162384b3e8848f4306dc0d4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}