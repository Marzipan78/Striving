{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd050a93f2fecfd00da27f8930a2c1c74e92db967b2162384b3e8848f4306dc0d4b",
   "display_name": "Python 3.7.10 64-bit ('deeplearner': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def view_classify(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5)) ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 251,
       "height": 248
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1,16,5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16,32,5),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2))\n",
    "        #self.layer3 = nn.Sequential(\n",
    "        #    nn.Conv2d(32,64,5),\n",
    "        #    #nn.BatchNorm2d(),\n",
    "        #    nn.ReLU(inplace=True),\n",
    "        #    nn.MaxPool2d(2)\n",
    "        #            )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 *4 *4,128),\n",
    "            nn.ReLU(inplace =True),\n",
    "            nn.Linear(128,32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32,10))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        #x = self.layer3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "y = images.resize_(images.shape[0], 1, 28,28)\n",
    "ps = model.forward(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (layer1): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=512, out_features=128, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Linear(in_features=128, out_features=32, bias=True)\n    (3): ReLU(inplace=True)\n    (4): Linear(in_features=32, out_features=10, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "#if torch.cuda.is_available():\n",
    " #   model = model.cuda()\n",
    " #   criterion = criterion.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/5\n",
      "\tIteration: 0\t Loss: 0.0460\n",
      "\tIteration: 50\t Loss: 1.8062\n",
      "\tIteration: 100\t Loss: 0.7599\n",
      "\tIteration: 150\t Loss: 0.3462\n",
      "\tIteration: 200\t Loss: 0.2296\n",
      "\tIteration: 250\t Loss: 0.1893\n",
      "\tIteration: 300\t Loss: 0.1668\n",
      "\tIteration: 350\t Loss: 0.1588\n",
      "\tIteration: 400\t Loss: 0.1347\n",
      "\tIteration: 450\t Loss: 0.1328\n",
      "\tIteration: 500\t Loss: 0.1254\n",
      "\tIteration: 550\t Loss: 0.1518\n",
      "\tIteration: 600\t Loss: 0.1285\n",
      "\tIteration: 650\t Loss: 0.1004\n",
      "\tIteration: 700\t Loss: 0.0990\n",
      "\tIteration: 750\t Loss: 0.0981\n",
      "\tIteration: 800\t Loss: 0.0923\n",
      "\tIteration: 850\t Loss: 0.0864\n",
      "\tIteration: 900\t Loss: 0.1137\n",
      "\tIteration: 950\t Loss: 0.0641\n",
      "\tIteration: 1000\t Loss: 0.0933\n",
      "\tIteration: 1050\t Loss: 0.0749\n",
      "\tIteration: 1100\t Loss: 0.1091\n",
      "\tIteration: 1150\t Loss: 0.0922\n",
      "\tIteration: 1200\t Loss: 0.0682\n",
      "\tIteration: 1250\t Loss: 0.1100\n",
      "\tIteration: 1300\t Loss: 0.0648\n",
      "\tIteration: 1350\t Loss: 0.0777\n",
      "\tIteration: 1400\t Loss: 0.0877\n",
      "\tIteration: 1450\t Loss: 0.0517\n",
      "\tIteration: 1500\t Loss: 0.0835\n",
      "\tIteration: 1550\t Loss: 0.0891\n",
      "\tIteration: 1600\t Loss: 0.0656\n",
      "\tIteration: 1650\t Loss: 0.0751\n",
      "\tIteration: 1700\t Loss: 0.0589\n",
      "\tIteration: 1750\t Loss: 0.0595\n",
      "\tIteration: 1800\t Loss: 0.0540\n",
      "\tIteration: 1850\t Loss: 0.0635\n",
      "Accuracy of the network: 98 %\n",
      "Epoch: 2/5\n",
      "\tIteration: 0\t Loss: 0.0006\n",
      "\tIteration: 50\t Loss: 0.0449\n",
      "\tIteration: 100\t Loss: 0.0482\n",
      "\tIteration: 150\t Loss: 0.0715\n",
      "\tIteration: 200\t Loss: 0.0457\n",
      "\tIteration: 250\t Loss: 0.0341\n",
      "\tIteration: 300\t Loss: 0.0511\n",
      "\tIteration: 350\t Loss: 0.0378\n",
      "\tIteration: 400\t Loss: 0.0501\n",
      "\tIteration: 450\t Loss: 0.0491\n",
      "\tIteration: 500\t Loss: 0.0482\n",
      "\tIteration: 550\t Loss: 0.0601\n",
      "\tIteration: 600\t Loss: 0.0393\n",
      "\tIteration: 650\t Loss: 0.0634\n",
      "\tIteration: 700\t Loss: 0.0458\n",
      "\tIteration: 750\t Loss: 0.0450\n",
      "\tIteration: 800\t Loss: 0.0429\n",
      "\tIteration: 850\t Loss: 0.0397\n",
      "\tIteration: 900\t Loss: 0.0640\n",
      "\tIteration: 950\t Loss: 0.0483\n",
      "\tIteration: 1000\t Loss: 0.0411\n",
      "\tIteration: 1050\t Loss: 0.0460\n",
      "\tIteration: 1100\t Loss: 0.0569\n",
      "\tIteration: 1150\t Loss: 0.0537\n",
      "\tIteration: 1200\t Loss: 0.0266\n",
      "\tIteration: 1250\t Loss: 0.0328\n",
      "\tIteration: 1300\t Loss: 0.0419\n",
      "\tIteration: 1350\t Loss: 0.0452\n",
      "\tIteration: 1400\t Loss: 0.0488\n",
      "\tIteration: 1450\t Loss: 0.0439\n",
      "\tIteration: 1500\t Loss: 0.0418\n",
      "\tIteration: 1550\t Loss: 0.0468\n",
      "\tIteration: 1600\t Loss: 0.0275\n",
      "\tIteration: 1650\t Loss: 0.0383\n",
      "\tIteration: 1700\t Loss: 0.0602\n",
      "\tIteration: 1750\t Loss: 0.0572\n",
      "\tIteration: 1800\t Loss: 0.0433\n",
      "\tIteration: 1850\t Loss: 0.0362\n",
      "Accuracy of the network: 98 %\n",
      "Epoch: 3/5\n",
      "\tIteration: 0\t Loss: 0.0000\n",
      "\tIteration: 50\t Loss: 0.0359\n",
      "\tIteration: 100\t Loss: 0.0228\n",
      "\tIteration: 150\t Loss: 0.0242\n",
      "\tIteration: 200\t Loss: 0.0091\n",
      "\tIteration: 250\t Loss: 0.0464\n",
      "\tIteration: 300\t Loss: 0.0217\n",
      "\tIteration: 350\t Loss: 0.0279\n",
      "\tIteration: 400\t Loss: 0.0393\n",
      "\tIteration: 450\t Loss: 0.0313\n",
      "\tIteration: 500\t Loss: 0.0384\n",
      "\tIteration: 550\t Loss: 0.0349\n",
      "\tIteration: 600\t Loss: 0.0447\n",
      "\tIteration: 650\t Loss: 0.0499\n",
      "\tIteration: 700\t Loss: 0.0159\n",
      "\tIteration: 750\t Loss: 0.0287\n",
      "\tIteration: 800\t Loss: 0.0314\n",
      "\tIteration: 850\t Loss: 0.0383\n",
      "\tIteration: 900\t Loss: 0.0390\n",
      "\tIteration: 950\t Loss: 0.0362\n",
      "\tIteration: 1000\t Loss: 0.0294\n",
      "\tIteration: 1050\t Loss: 0.0356\n",
      "\tIteration: 1100\t Loss: 0.0448\n",
      "\tIteration: 1150\t Loss: 0.0331\n",
      "\tIteration: 1200\t Loss: 0.0283\n",
      "\tIteration: 1250\t Loss: 0.0225\n",
      "\tIteration: 1300\t Loss: 0.0228\n",
      "\tIteration: 1350\t Loss: 0.0292\n",
      "\tIteration: 1400\t Loss: 0.0332\n",
      "\tIteration: 1450\t Loss: 0.0263\n",
      "\tIteration: 1500\t Loss: 0.0306\n",
      "\tIteration: 1550\t Loss: 0.0352\n",
      "\tIteration: 1600\t Loss: 0.0375\n",
      "\tIteration: 1650\t Loss: 0.0382\n",
      "\tIteration: 1700\t Loss: 0.0318\n",
      "\tIteration: 1750\t Loss: 0.0240\n",
      "\tIteration: 1800\t Loss: 0.0274\n",
      "\tIteration: 1850\t Loss: 0.0276\n",
      "Accuracy of the network: 99 %\n",
      "Epoch: 4/5\n",
      "\tIteration: 0\t Loss: 0.0000\n",
      "\tIteration: 50\t Loss: 0.0194\n",
      "\tIteration: 100\t Loss: 0.0098\n",
      "\tIteration: 150\t Loss: 0.0175\n",
      "\tIteration: 200\t Loss: 0.0233\n",
      "\tIteration: 250\t Loss: 0.0240\n",
      "\tIteration: 300\t Loss: 0.0242\n",
      "\tIteration: 350\t Loss: 0.0297\n",
      "\tIteration: 400\t Loss: 0.0210\n",
      "\tIteration: 450\t Loss: 0.0173\n",
      "\tIteration: 500\t Loss: 0.0076\n",
      "\tIteration: 550\t Loss: 0.0133\n",
      "\tIteration: 600\t Loss: 0.0249\n",
      "\tIteration: 650\t Loss: 0.0299\n",
      "\tIteration: 700\t Loss: 0.0330\n",
      "\tIteration: 750\t Loss: 0.0172\n",
      "\tIteration: 800\t Loss: 0.0342\n",
      "\tIteration: 850\t Loss: 0.0167\n",
      "\tIteration: 900\t Loss: 0.0173\n",
      "\tIteration: 950\t Loss: 0.0360\n",
      "\tIteration: 1000\t Loss: 0.0330\n",
      "\tIteration: 1050\t Loss: 0.0302\n",
      "\tIteration: 1100\t Loss: 0.0234\n",
      "\tIteration: 1150\t Loss: 0.0240\n",
      "\tIteration: 1200\t Loss: 0.0356\n",
      "\tIteration: 1250\t Loss: 0.0300\n",
      "\tIteration: 1300\t Loss: 0.0174\n",
      "\tIteration: 1350\t Loss: 0.0142\n",
      "\tIteration: 1400\t Loss: 0.0349\n",
      "\tIteration: 1450\t Loss: 0.0244\n",
      "\tIteration: 1500\t Loss: 0.0236\n",
      "\tIteration: 1550\t Loss: 0.0369\n",
      "\tIteration: 1600\t Loss: 0.0186\n",
      "\tIteration: 1650\t Loss: 0.0297\n",
      "\tIteration: 1700\t Loss: 0.0100\n",
      "\tIteration: 1750\t Loss: 0.0158\n",
      "\tIteration: 1800\t Loss: 0.0227\n",
      "\tIteration: 1850\t Loss: 0.0337\n",
      "Accuracy of the network: 98 %\n",
      "Epoch: 5/5\n",
      "\tIteration: 0\t Loss: 0.0017\n",
      "\tIteration: 50\t Loss: 0.0095\n",
      "\tIteration: 100\t Loss: 0.0111\n",
      "\tIteration: 150\t Loss: 0.0121\n",
      "\tIteration: 200\t Loss: 0.0117\n",
      "\tIteration: 250\t Loss: 0.0135\n",
      "\tIteration: 300\t Loss: 0.0136\n",
      "\tIteration: 350\t Loss: 0.0211\n",
      "\tIteration: 400\t Loss: 0.0282\n",
      "\tIteration: 450\t Loss: 0.0200\n",
      "\tIteration: 500\t Loss: 0.0141\n",
      "\tIteration: 550\t Loss: 0.0141\n",
      "\tIteration: 600\t Loss: 0.0284\n",
      "\tIteration: 650\t Loss: 0.0151\n",
      "\tIteration: 700\t Loss: 0.0154\n",
      "\tIteration: 750\t Loss: 0.0177\n",
      "\tIteration: 800\t Loss: 0.0121\n",
      "\tIteration: 850\t Loss: 0.0112\n",
      "\tIteration: 900\t Loss: 0.0244\n",
      "\tIteration: 950\t Loss: 0.0186\n",
      "\tIteration: 1000\t Loss: 0.0292\n",
      "\tIteration: 1050\t Loss: 0.0244\n",
      "\tIteration: 1100\t Loss: 0.0376\n",
      "\tIteration: 1150\t Loss: 0.0220\n",
      "\tIteration: 1200\t Loss: 0.0235\n",
      "\tIteration: 1250\t Loss: 0.0272\n",
      "\tIteration: 1300\t Loss: 0.0359\n",
      "\tIteration: 1350\t Loss: 0.0224\n",
      "\tIteration: 1400\t Loss: 0.0204\n",
      "\tIteration: 1450\t Loss: 0.0057\n",
      "\tIteration: 1500\t Loss: 0.0089\n",
      "\tIteration: 1550\t Loss: 0.0256\n",
      "\tIteration: 1600\t Loss: 0.0119\n",
      "\tIteration: 1650\t Loss: 0.0194\n",
      "\tIteration: 1700\t Loss: 0.0152\n",
      "\tIteration: 1750\t Loss: 0.0138\n",
      "\tIteration: 1800\t Loss: 0.0296\n",
      "\tIteration: 1850\t Loss: 0.0189\n",
      "Accuracy of the network: 99 %\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "epochs = 5\n",
    "print_every = 50\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 1,28,28)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(iter(testloader)):\n",
    "            images.resize_(images.size()[0], 1,28,28)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ -8.3042,  -0.0705,   2.5154,   2.0810,  -0.8464,  -2.6962, -10.0038,\n",
       "          15.0043,  -2.5489,   3.7208],\n",
       "        [ -5.7439,  15.8543,   2.6313,  -5.4803,   1.4986,  -5.5608,  -2.9043,\n",
       "           2.5956,  -4.0300,  -7.3062],\n",
       "        [ -5.7888,  -0.9107,  -0.1905,  -5.7614,  11.5377,  -1.3167,  -1.6169,\n",
       "          -1.5485,   1.6785,   1.9628],\n",
       "        [ 21.3662, -10.4032,  -1.1537,  -4.4704,  -1.3680,  -1.7140,   0.0744,\n",
       "          -3.9889,   0.2632,   0.8772],\n",
       "        [ -4.7131,   2.0111,  -0.1071,  -0.1131,  -0.5054,  -2.9665,  -5.8046,\n",
       "           9.9371,  -2.0335,   1.7600],\n",
       "        [ -4.5378,   2.6891,  15.8350,   1.8013,   0.7695,  -7.5511,  -6.9438,\n",
       "           6.6657,  -0.2362, -10.5144],\n",
       "        [  0.0672,  -2.4722,  -1.8344,  -6.0406,  -2.6505,   1.9825,  18.9992,\n",
       "         -14.7044,  -1.5722,  -5.7785],\n",
       "        [ -3.9465,   2.0818,   3.3067,  -0.8691,  -1.5486,  -4.4033,  -6.4683,\n",
       "          11.0656,  -2.4215,   0.2393],\n",
       "        [ -8.4889,  -2.2969,   0.2933,  -6.6351,  16.7720,  -1.3909,  -2.3800,\n",
       "          -2.1653,   1.6590,  -0.1959],\n",
       "        [ -7.4773,  -2.8432,  -0.1397,  -8.7483,  17.1304,  -1.2909,   0.1671,\n",
       "          -5.0073,   0.9232,   3.4379],\n",
       "        [ -2.0196,  -5.0224, -10.0209,   1.4602,  -4.4530,  17.3753,   1.7497,\n",
       "          -2.0584,   3.5336,   4.7731],\n",
       "        [ -4.1957,  -1.4835,  -3.5767,  -0.8345,   2.3613,  -3.2073,  -4.3157,\n",
       "           2.7571,   5.6083,  10.3766],\n",
       "        [ -9.0589,  -2.8380,  -7.6975,  -1.6891,   6.1621,  -2.0056,  -7.1323,\n",
       "           4.7958,   2.1225,  21.9328],\n",
       "        [ -1.8463,  -2.8469,  -0.9357,  -1.5377,  -1.3298,  -1.9320,  -2.5881,\n",
       "          -1.8838,  17.2746,   3.3900],\n",
       "        [ -5.2920,   2.4075,   0.9288,  -1.9279,  -1.0764,  -4.1476,  -7.3649,\n",
       "          14.6251,  -3.1139,   0.3394],\n",
       "        [ -5.8478,  -0.7385,   3.7368,   0.8905,  -1.5905,  -3.0683,  -8.7613,\n",
       "          12.9282,  -2.6225,   4.6700],\n",
       "        [ 11.0484,  -6.1886,  -0.5884,  -2.4386,  -0.7885,  -1.5814,   2.9846,\n",
       "          -4.8169,   0.2549,   1.2594],\n",
       "        [ -1.9045,  -4.5983,  -7.5795,   5.2493,  -5.8444,  14.3676,   0.2979,\n",
       "          -3.1787,   1.6194,   2.3117],\n",
       "        [ -1.5750,  -1.0105,  -2.0536,  -0.7184,  -1.3977,  -0.4549,  -1.0992,\n",
       "          -1.7423,  11.9673,   1.0182],\n",
       "        [ -4.0511,  15.5021,  -1.8082,  -6.3555,   0.3085,  -2.8345,  -4.5383,\n",
       "           2.4966,   0.2750,  -2.2602],\n",
       "        [ -6.5507,  -3.5673,  -6.2643,  -1.4963,   4.9055,  -2.6243,  -5.2530,\n",
       "           2.3334,   4.2408,  16.5356],\n",
       "        [ -9.2705,  -4.6706,  -6.3195,  23.6447, -10.2849,   1.5821, -10.3042,\n",
       "          -0.5941,   1.3486,   3.1091],\n",
       "        [ 19.3884,  -6.9806,  -1.5966,  -4.0041,  -3.3147,  -0.5530,  -0.3313,\n",
       "          -3.1798,   0.9842,  -2.6893],\n",
       "        [ -0.0805,  -2.3695,  -3.1807,  -5.4291,  -3.3274,   3.2636,  16.8359,\n",
       "         -13.9214,   1.7185,  -4.4269],\n",
       "        [ -6.1519,  -8.2297,  -7.0267,  22.3047, -11.8798,   5.2539,  -7.4038,\n",
       "          -2.9344,   5.2000,   1.1124],\n",
       "        [ -3.5610,  -0.3299,   0.6293,   1.0364,  -2.0669,  -2.4363,  -5.2059,\n",
       "           8.3946,  -1.6815,   2.3345],\n",
       "        [ -6.1467,  -1.4566,  -1.6051,  -7.6547,  15.0717,  -1.8991,  -0.9593,\n",
       "          -3.1979,   0.3061,   5.6479],\n",
       "        [ 12.3264,  -5.5755,   2.2736,  -3.5502,  -0.8476,  -3.5882,  -2.1845,\n",
       "           1.4677,  -1.7480,   0.7295],\n",
       "        [ 20.6388,  -9.1340,  -1.9539,  -4.8910,  -0.9472,  -2.5266,   1.8604,\n",
       "          -5.2797,  -0.3354,   0.3811],\n",
       "        [ -4.1651,  14.6101,  -2.8903,  -8.5430,   0.5544,  -3.0626,   1.6947,\n",
       "          -2.5239,   2.3100,  -4.9032],\n",
       "        [ 22.3520, -12.1008,   2.2683,  -4.7830,  -0.2721,  -2.8284,   0.4379,\n",
       "          -4.5175,  -2.1495,   3.9924],\n",
       "        [ -4.1087,  16.5197,  -2.0055,  -6.9456,   0.5739,  -3.0971,  -5.0176,\n",
       "           2.4989,   0.4013,  -2.3932]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "y =images.resize_(images.shape[0],1, 28,28)\n",
    "\n",
    "ps = model.forward(y)\n",
    "ps.shape\n",
    "pred = torch.exp(ps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -0.2706,  0.6471,  0.3647, -0.9529,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -0.2235,  0.9373,  0.9765,  0.9765, -0.8902,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.9608, -0.7412,  0.2863, -0.1216, -0.9686, -1.0000, -1.0000,\n",
       "          -1.0000, -0.9922,  0.0039,  0.9765,  0.9765,  0.9765, -0.8902,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.4118,  0.9765,  0.9765,  0.9765, -0.5373, -1.0000, -1.0000,\n",
       "          -1.0000, -0.8824,  0.9765,  0.9765,  0.9765,  0.9765, -0.8902,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.0667,  0.9765,  0.9765,  0.9765, -0.5373, -1.0000, -1.0000,\n",
       "          -1.0000, -0.5294,  0.9765,  0.9765,  0.9765,  0.9765, -0.8902,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           0.7490,  0.9765,  0.9765,  0.9765, -0.5373, -1.0000, -1.0000,\n",
       "          -1.0000,  0.2863,  0.9765,  0.9765,  0.9765,  0.9765, -0.8902,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           0.7490,  0.9765,  0.9765,  0.9765, -0.5373, -1.0000, -1.0000,\n",
       "          -1.0000,  0.2863,  0.9765,  0.9765,  0.9765,  0.9765, -0.8902,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1137,\n",
       "           0.9608,  0.9765,  0.9765,  0.9765, -0.0039, -0.2941, -1.0000,\n",
       "          -0.8275,  0.5294,  0.9765,  0.9765,  0.9765,  0.9765, -0.8902,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9765,  0.1137,\n",
       "           0.9765,  0.9765,  0.9765,  0.9765,  0.9765,  0.9765,  0.5216,\n",
       "          -0.0588,  0.9765,  0.9765,  0.9765,  0.9765,  0.7882, -0.9059,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8824,  0.9765,\n",
       "           0.9765,  0.9765,  0.9765,  0.9765,  0.9765,  0.9765,  0.9843,\n",
       "           0.9765,  0.9765,  0.9765,  0.9765,  0.9765, -0.0745, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.2863,  0.9843,\n",
       "           0.9843,  0.9843,  0.9843,  0.2078,  0.1686,  0.1686,  1.0000,\n",
       "           0.9843,  0.9843,  0.9843,  0.9843,  0.8902, -0.4510, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7725, -0.6549,\n",
       "           0.3725,  0.5059, -0.1137, -0.9765, -1.0000, -1.0000, -0.1686,\n",
       "           0.8902,  0.9765,  0.9765,  0.9765,  0.7412, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           0.6314,  0.9765,  0.9765,  0.9765,  0.6706, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           0.6314,  0.9765,  0.9765,  0.9765, -0.4196, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           0.6314,  0.9765,  0.9765,  0.9765, -0.4196, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4118,\n",
       "           0.8745,  0.9765,  0.9765,  0.9765, -0.4196, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1765,\n",
       "           0.9765,  0.9765,  0.9765,  0.9765, -0.4196, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1765,\n",
       "           0.9765,  0.9765,  0.9765,  0.9765, -0.4196, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3255,\n",
       "           0.9137,  0.9765,  0.9765,  0.5216, -0.8667, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.0510,  0.9765,  0.9765,  0.3882, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "y[0].view(1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}